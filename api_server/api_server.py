# File: api_server.py

import asyncio
import json
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sse_starlette.sse import EventSourceResponse
import logging

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("api_server")

# Import your existing agent creator
from agent import create_agent_executor

def ultra_fast_response(query: str) -> str | None:
    """Ultra-fast responses for common queries without any agent overhead."""
    query_lower = query.lower().strip()
    
    # Direct cached responses for super common queries
    instant_responses = {
        "hello": "Hello! How can I help you today?",
        "hi": "Hi there! What can I assist you with?",
        "how are you": "I'm doing well, thank you! How can I help you?",
        "what is your name": "I'm Privy, your AI assistant!",
        "who are you": "I'm Privy, an AI assistant designed to help you find information.",
    }
    
    if query_lower in instant_responses:
        return instant_responses[query_lower]
    
    # For search queries, don't return ultra-fast responses - let the agent handle them
    # This prevents bypassing the search tool
    return None

# Initialize the agent executor once on startup
logger.debug("ðŸ”„ Initializing agent executor...")
agent_executor = create_agent_executor()
logger.debug("âœ… Agent executor initialized")

# --- FastAPI App Setup ---
app = FastAPI()

# Configure CORS to allow your Blazor app to connect
# IMPORTANT: In production, you should restrict the origins.
# Configure CORS to allow your extension to connect

# origins = [
#     "http://localhost:5000",
#     "https://localhost:5001", 
#     "http://localhost:5199",
#     "https://localhost:7155",
#     "chrome-extension://*",  # Chrome extensions
#     "moz-extension://*",     # Firefox extensions
# ]
origins = [
    "http://localhost:5000",
    "https://localhost:5001", 
    "http://localhost:5199",
    "https://localhost:7155",
    # Add these for development:
    "*"  # Allow all (easiest for development)
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Define the request model to match what Blazor will send
class ChatRequest(BaseModel):
    input: str

# --- Streaming Event Generator ---
async def stream_agent_response(chat_request: ChatRequest):
    """
    Calls the agent's astream_events and yields formatted Server-Sent Events.
    """
    logger.debug(f"ðŸŽ¯ Starting stream for query: '{chat_request.input}'")
    
    # ULTRA-FAST PRE-CHECK
    fast_response = ultra_fast_response(chat_request.input)
    if fast_response:
        logger.debug(f"ðŸš€ ULTRA-FAST response for: '{chat_request.input}'")
        # Yield the fast response immediately
        yield {
            "event": "message",
            "data": json.dumps({"type": "token", "content": fast_response})
        }
        return


    event_count = 0
    try:
        async for event in agent_executor.astream_events(
            {"input": chat_request.input},
            version="v1",
        ):
            event_count += 1
            kind = event["event"]
            data_payload = {}
            
            logger.debug(f"ðŸ”„ Event {event_count}: {kind}")

            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    data_payload = {"type": "token", "content": content}
                    logger.debug(f"ðŸ’¬ Token: {content[:50]}...")
            
            elif kind == "on_tool_start":
                tool_name = event['name']
                tool_input = event['data'].get('input')
                data_payload = {
                    "type": "tool_start", 
                    "name": tool_name,
                    "input": tool_input
                }
                logger.debug(f"ðŸ› ï¸ Tool START: {tool_name} with input: {str(tool_input)[:100]}...")
        
            elif kind == "on_tool_end":
                tool_name = event['name']
                tool_output = event['data'].get('output', '')
                data_payload = {
                    "type": "tool_end",
                    "name": tool_name,
                    "output": tool_output
                }
                logger.debug(f"ðŸ› ï¸ Tool END: {tool_name} with output length: {len(str(tool_output))}")
                logger.debug(f"ðŸ› ï¸ Tool output sample: {str(tool_output)[:200]}...")

            elif kind == "on_chain_start":
                chain_name = event["name"]
                # logger.debug(f"â›“ï¸ Chain START: {chain_name}")

            elif kind == "on_chain_end":
                chain_name = event["name"]
                # logger.debug(f"â›“ï¸ Chain END: {chain_name}")
                if chain_name == "AgentExecutor":
                    output_data = event['data'].get('output', {})
                    # logger.debug(f"ðŸ AgentExecutor finished with output: {str(output_data)[:200]}...")
            
            # Yield the event if it has content
            if data_payload:
                yield {
                    "event": "message",
                    "data": json.dumps(data_payload)
                }
                
        logger.debug(f"âœ… Stream completed. Total events: {event_count}")

    except Exception as e:
        logger.error(f"âŒ Error in stream_agent_response: {e}")
        import traceback
        logger.error(f"âŒ Traceback: {traceback.format_exc()}")
        
        # Yield an error event to the client
        error_payload = {
            "type": "error",
            "content": f"An error occurred: {str(e)}"
        }
        yield {
            "event": "message",
            "data": json.dumps(error_payload)
        }

# --- API Endpoint ---
@app.post("/agent-chat")
async def chat_endpoint(chat_request: ChatRequest):
    """
    The main chat endpoint that Blazor will call.
    """
    logger.debug(f"ðŸ“¥ Received chat request: '{chat_request.input}'")
    return EventSourceResponse(stream_agent_response(chat_request))


@app.post("/summarize")
async def summarize_page(request: dict):
    """
    Directly summarize webpage content.
    """
    content = request.get("content", "")
    url = request.get("url", "")
    title = request.get("title", "")
    
    logger.info(f"ðŸ“„ Summarize request for: {title}")
    
    async def summarize_generator():
        try:
            # Import and get the LLM
            from agent import get_llm
            
            llm = get_llm()  # This will create it if needed
            logger.info(f"âœ“ Got LLM: {llm}")
            
            # Create prompt
            prompt = f"""Summarize the following webpage in a clear, structured format.

Title: {title}
URL: {url}

Content:
{content[:8000]}

Format your summary like this:

ðŸ“Œ Overview:
[One sentence description]

ðŸ”‘ Key Points:
â€¢ [Main point 1]
â€¢ [Main point 2]
â€¢ [Main point 3]

ðŸ’¡ Takeaway:
[Brief conclusion]

Keep it concise and scannable and in ENGLISH"""

            logger.info("ðŸ”„ Starting LLM stream...")
            
            # Stream the response
            async for chunk in llm.astream(prompt):
                if hasattr(chunk, 'content') and chunk.content:
                    yield {
                        "event": "message",
                        "data": json.dumps({
                            "type": "token",
                            "content": chunk.content
                        })
                    }
                    
            logger.info("âœ… Summary completed")
                    
        except Exception as e:
            logger.error(f"âŒ Summarization error: {e}")
            import traceback
            logger.error(f"Traceback:\n{traceback.format_exc()}")
            
            yield {
                "event": "message",
                "data": json.dumps({
                    "type": "error",
                    "content": f"Summarization failed: {str(e)}"
                })
            }
    
    return EventSourceResponse(summarize_generator())

@app.get("/health")
async def health_check():
    """Health check endpoint to verify the API is running."""
    return {"status": "healthy", "service": "cluj-ai-api"}

@app.get("/tools")
async def list_tools():
    """Debug endpoint to list available tools."""
    try:
        from tools import ALL_TOOLS
        tools_info = []
        for tool in ALL_TOOLS:
            tools_info.append({
                "name": tool.name,
                "description": tool.description,
                "args": str(tool.args)
            })
        return {"tools": tools_info}
    except Exception as e:
        return {"error": str(e)}

@app.get("/test-search")
async def test_search():
    """Test the search tool directly"""
    try:
        from tools import search_tool
        result = search_tool("current president of the United States")
        return {
            "status": "search_tool test completed",
            "result_length": len(result),
            "result_preview": result[:500] + "..." if len(result) > 500 else result
        }
    except Exception as e:
        return {"status": "search_tool test failed", "error": str(e)}

if __name__ == "__main__":
    import uvicorn
    # logger.info("ðŸš€ Starting API server on http://127.0.0.1:8000")
    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="debug")