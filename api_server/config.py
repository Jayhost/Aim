# config.py

# This is now just a label for the model being used by the server.
# The actual model is determined by the command used to start the llama.cpp server.
# MODEL_NAME = "Qwen3-30B-A3B-Instruct-2507-Q3_K_M"
MODEL_NAME = "Qwen3-4B-Instruct-2507-Q4_K_S"

YT_DLP_PATH = "/usr/bin/yt-dlp"
MPV_PATH = "/usr/bin/mpv"    
