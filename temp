api-server-1  | INFO:     Started server process [1]
api-server-1  | INFO:     Waiting for application startup.
api-server-1  | INFO:     Application startup complete.
api-server-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
api-server-1  | INFO:     172.20.0.1:55808 - "POST /agent-chat HTTP/1.1" 200 OK
api-server-1  | INFO:     172.20.0.1:55808 - "POST /agent-chat HTTP/1.1" 200 OK
api-server-1  | INFO:     Shutting down
api-server-1  | INFO:     Waiting for application shutdown.
api-server-1  | INFO:     Application shutdown complete.
api-server-1  | INFO:     Finished server process [1]
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:05:14,322 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:05:14,354 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:05:14,359 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:05:14,361 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:05:14,362 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:10:17,918 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:10:17,994 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:10:18,023 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:10:18,030 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:10:18,035 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.KeyManagement.XmlKeyManager[35]
blazor-app-1  |       No XML encryptor configured. Key {9ed1f4b4-c47b-4206-b27a-d44faf79467e} may be persisted to storage in unencrypted form.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | fail: Microsoft.AspNetCore.Antiforgery.DefaultAntiforgery[7]
blazor-app-1  |       An exception was thrown while deserializing the token.
blazor-app-1  |       Microsoft.AspNetCore.Antiforgery.AntiforgeryValidationException: The antiforgery token could not be decrypted.
blazor-app-1  |        ---> System.Security.Cryptography.CryptographicException: The key {48dc6cdf-8849-41db-9db8-64e0796f57a5} was not found in the key ring. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  |          at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.UnprotectCore(Byte[] protectedData, Boolean allowOperationsOnRevokedKeys, UnprotectStatus& status)
blazor-app-1  |          at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.Unprotect(Byte[] protectedData)
blazor-app-1  |          at Microsoft.AspNetCore.Antiforgery.DefaultAntiforgeryTokenSerializer.Deserialize(String serializedToken)
blazor-app-1  |          --- End of inner exception stack trace ---
llama-1       | ggml_vulkan: No devices found.
llama-1       | load_backend: loaded Vulkan backend from /app/libggml-vulkan.so
llama-1       | load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
llama-1       | build: 6265 (c247d06f) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu
llama-1       | system info: n_threads = 6, n_threads_batch = 6, total_threads = 12
llama-1       | 
llama-1       | system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
llama-1       | 
llama-1       | main: binding port with default address family
llama-1       | main: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 11
llama-1       | main: loading model
llama-1       | srv    load_model: loading model '/models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf'
llama-1       | llama_model_loader: loaded meta data with 42 key-value pairs and 398 tensors from /models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf (version GGUF V3 (latest))
llama-1       | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama-1       | llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama-1       | llama_model_loader: - kv   1:                               general.type str              = model
llama-1       | llama_model_loader: - kv   2:                               general.name str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   3:                            general.version str              = 2507
llama-1       | llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama-1       | llama_model_loader: - kv   5:                           general.basename str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth
blazor-app-1  |          at Microsoft.AspNetCore.Antiforgery.DefaultAntiforgeryTokenSerializer.Deserialize(String serializedToken)
blazor-app-1  |          at Microsoft.AspNetCore.Antiforgery.DefaultAntiforgery.GetCookieTokenDoesNotThrow(HttpContext httpContext)
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 24.2352ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 33.1458ms - 200
blazor-app-1  | An exception occurred: System.Net.Http.HttpIOException: The response ended prematurely. (ResponseEnded)
blazor-app-1  |    at System.Net.Http.HttpConnection.FillAsync(Boolean async)
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Fill()
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Read(Span`1 buffer)
blazor-app-1  |    at System.IO.StreamReader.ReadBuffer()
blazor-app-1  |    at System.IO.StreamReader.get_EndOfStream()
blazor-app-1  |    at MyBlazorApp.Components.Pages.Chat.SendPrompt() in /src/Components/Pages/Chat.razor:line 77
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 2.7483ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 2.8894ms - 200
blazor-app-1  | An exception occurred: System.Net.Http.HttpIOException: The response ended prematurely. (ResponseEnded)
blazor-app-1  |    at System.Net.Http.HttpConnection.FillAsync(Boolean async)
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Fill()
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Read(Span`1 buffer)
blazor-app-1  |    at System.IO.StreamReader.ReadBuffer()
blazor-app-1  |    at System.IO.StreamReader.get_EndOfStream()
blazor-app-1  |    at MyBlazorApp.Components.Pages.Chat.SendPrompt() in /src/Components/Pages/Chat.razor:line 77
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 4.7107ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 4.8244ms - 200
blazor-app-1  | An exception occurred: System.Net.Http.HttpIOException: The response ended prematurely. (ResponseEnded)
blazor-app-1  |    at System.Net.Http.HttpConnection.FillAsync(Boolean async)
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Fill()
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Read(Span`1 buffer)
blazor-app-1  |    at System.IO.StreamReader.ReadBuffer()
blazor-app-1  |    at System.IO.StreamReader.get_EndOfStream()
blazor-app-1  |    at MyBlazorApp.Components.Pages.Chat.SendPrompt() in /src/Components/Pages/Chat.razor:line 77
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 4.9668ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 5.219ms - 200
blazor-app-1  | An exception occurred: System.Net.Http.HttpIOException: The response ended prematurely. (ResponseEnded)
blazor-app-1  |    at System.Net.Http.HttpConnection.FillAsync(Boolean async)
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Fill()
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Read(Span`1 buffer)
blazor-app-1  |    at System.IO.StreamReader.ReadBuffer()
blazor-app-1  |    at System.IO.StreamReader.get_EndOfStream()
blazor-app-1  |    at MyBlazorApp.Components.Pages.Chat.SendPrompt() in /src/Components/Pages/Chat.razor:line 77
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 24.7447ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 33.885ms - 200
blazor-app-1  | An exception occurred: System.Net.Http.HttpIOException: The response ended prematurely. (ResponseEnded)
blazor-app-1  |    at System.Net.Http.HttpConnection.FillAsync(Boolean async)
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Fill()
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Read(Span`1 buffer)
blazor-app-1  |    at System.IO.StreamReader.ReadBuffer()
blazor-app-1  |    at System.IO.StreamReader.get_EndOfStream()
blazor-app-1  |    at MyBlazorApp.Components.Pages.Chat.SendPrompt() in /src/Components/Pages/Chat.razor:line 77
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 22.4721ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 31.7094ms - 200
blazor-app-1  | An exception occurred: System.Net.Http.HttpIOException: The response ended prematurely. (ResponseEnded)
blazor-app-1  |    at System.Net.Http.HttpConnection.FillAsync(Boolean async)
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Fill()
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Read(Span`1 buffer)
blazor-app-1  |    at System.IO.StreamReader.ReadBuffer()
blazor-app-1  |    at System.IO.StreamReader.get_EndOfStream()
blazor-app-1  |    at MyBlazorApp.Components.Pages.Chat.SendPrompt() in /src/Components/Pages/Chat.razor:line 77
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 4.7572ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 4.8726ms - 200
blazor-app-1  | An exception occurred: System.Net.Http.HttpIOException: The response ended prematurely. (ResponseEnded)
blazor-app-1  |    at System.Net.Http.HttpConnection.FillAsync(Boolean async)
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Fill()
blazor-app-1  |    at System.Net.Http.HttpConnection.ChunkedEncodingReadStream.Read(Span`1 buffer)
blazor-app-1  |    at System.IO.StreamReader.ReadBuffer()
blazor-app-1  |    at System.IO.StreamReader.get_EndOfStream()
blazor-app-1  |    at MyBlazorApp.Components.Pages.Chat.SendPrompt() in /src/Components/Pages/Chat.razor:line 77
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 26.1797ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 35.3049ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
searxng-1     | [INFO] Started worker-1 runtime-1
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:13:00,208 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:13:00,238 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:13:00,246 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:13:00,248 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:13:00,250 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:15:34,467 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:15:34,491 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
llama-1       | llama_model_loader: - kv   7:                         general.size_label str              = 4B
llama-1       | llama_model_loader: - kv   8:                            general.license str              = apache-2.0
llama-1       | llama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  10:                           general.repo_url str              = https://huggingface.co/unsloth
llama-1       | llama_model_loader: - kv  11:                   general.base_model.count u32              = 1
llama-1       | llama_model_loader: - kv  12:                  general.base_model.0.name str              = Qwen3 4B Instruct 2507
llama-1       | llama_model_loader: - kv  13:               general.base_model.0.version str              = 2507
llama-1       | llama_model_loader: - kv  14:          general.base_model.0.organization str              = Qwen
llama-1       | llama_model_loader: - kv  15:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  16:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
searxng-1     | 2025-10-07 10:15:34,497 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:15:34,499 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:15:34,500 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
llama-1       | llama_model_loader: - kv  17:                          qwen3.block_count u32              = 36
llama-1       | llama_model_loader: - kv  18:                       qwen3.context_length u32              = 262144
llama-1       | llama_model_loader: - kv  19:                     qwen3.embedding_length u32              = 2560
llama-1       | llama_model_loader: - kv  20:                  qwen3.feed_forward_length u32              = 9728
llama-1       | llama_model_loader: - kv  21:                 qwen3.attention.head_count u32              = 32
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
llama-1       | llama_model_loader: - kv  22:              qwen3.attention.head_count_kv u32              = 8
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:17:22,032 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:17:22,072 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
searxng-1     | 2025-10-07 10:17:22,078 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:17:22,080 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:17:22,082 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 2.8166ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 2.9615ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 2.1574ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 2.2905ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
llama-1       | llama_model_loader: - kv  23:                       qwen3.rope.freq_base f32              = 5000000.000000
llama-1       | llama_model_loader: - kv  24:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama-1       | llama_model_loader: - kv  25:                 qwen3.attention.key_length u32              = 128
llama-1       | llama_model_loader: - kv  26:               qwen3.attention.value_length u32              = 128
llama-1       | llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama-1       | llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama-1       | llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama-1       | llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama-1       | llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama-1       | llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645
llama-1       | llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151654
llama-1       | llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false
llama-1       | llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama-1       | llama_model_loader: - kv  36:               general.quantization_version u32              = 2
llama-1       | llama_model_loader: - kv  37:                          general.file_type u32              = 14
llama-1       | llama_model_loader: - kv  38:                      quantize.imatrix.file str              = Qwen3-4B-Instruct-2507-GGUF/imatrix_u...
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 2.0962ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 2.2055ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
llama-1       | llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-4B-Instruct...
llama-1       | llama_model_loader: - kv  40:             quantize.imatrix.entries_count u32              = 252
llama-1       | llama_model_loader: - kv  41:              quantize.imatrix.chunks_count u32              = 79
llama-1       | llama_model_loader: - type  f32:  145 tensors
llama-1       | llama_model_loader: - type q4_K:  244 tensors
llama-1       | llama_model_loader: - type q5_K:    8 tensors
llama-1       | llama_model_loader: - type q6_K:    1 tensors
llama-1       | print_info: file format = GGUF V3 (latest)
llama-1       | print_info: file type   = Q4_K - Small
llama-1       | print_info: file size   = 2.21 GiB (4.73 BPW) 
llama-1       | load: printing all EOG tokens:
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 1.1526ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 1.241ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 2.1603ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 2.2893ms - 200
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:17:46,998 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:17:47,020 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:17:47,025 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:17:47,028 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 25.4271ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
searxng-1     | 2025-10-07 10:17:47,030 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
llama-1       | load:   - 151643 ('<|endoftext|>')
llama-1       | load:   - 151645 ('<|im_end|>')
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:18:07,282 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:18:07,305 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:18:07,312 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:18:07,314 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:18:07,316 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | 2025-10-07 10:18:25,755 ERROR:searx.botdetection: X-Forwarded-For nor X-Real-IP header is set!
searxng-1     | [INFO] Shutting down granian
blazor-app-1  |       End processing HTTP request after 34.2492ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 1.8286ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 2.0229ms - 200
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
llama-1       | load:   - 151662 ('<|fim_pad|>')
llama-1       | load:   - 151663 ('<|repo_name|>')
llama-1       | load:   - 151664 ('<|file_sep|>')
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
llama-1       | load: special tokens cache size = 26
llama-1       | load: token to piece cache size = 0.9311 MB
llama-1       | print_info: arch             = qwen3
llama-1       | print_info: vocab_only       = 0
llama-1       | print_info: n_ctx_train      = 262144
llama-1       | print_info: n_embd           = 2560
llama-1       | print_info: n_layer          = 36
llama-1       | print_info: n_head           = 32
llama-1       | print_info: n_head_kv        = 8
llama-1       | print_info: n_rot            = 128
llama-1       | print_info: n_swa            = 0
llama-1       | print_info: is_swa_any       = 0
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:21:35,802 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:21:35,824 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:21:35,830 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:21:35,832 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:21:35,834 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
llama-1       | print_info: n_embd_head_k    = 128
llama-1       | print_info: n_embd_head_v    = 128
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:23:39,946 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:23:40,071 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:23:40,082 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:23:40,084 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:23:40,087 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
llama-1       | print_info: n_gqa            = 4
llama-1       | print_info: n_embd_k_gqa     = 1024
llama-1       | print_info: n_embd_v_gqa     = 1024
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
llama-1       | print_info: f_norm_eps       = 0.0e+00
llama-1       | print_info: f_norm_rms_eps   = 1.0e-06
llama-1       | print_info: f_clamp_kqv      = 0.0e+00
llama-1       | print_info: f_max_alibi_bias = 0.0e+00
llama-1       | print_info: f_logit_scale    = 0.0e+00
llama-1       | print_info: f_attn_scale     = 0.0e+00
llama-1       | print_info: n_ff             = 9728
llama-1       | print_info: n_expert         = 0
llama-1       | print_info: n_expert_used    = 0
llama-1       | print_info: causal attn      = 1
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 25.8747ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
llama-1       | print_info: pooling type     = -1
llama-1       | print_info: rope type        = 2
llama-1       | print_info: rope scaling     = linear
llama-1       | print_info: freq_base_train  = 5000000.0
llama-1       | print_info: freq_scale_train = 1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:26:16,938 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:26:17,002 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:26:17,011 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
llama-1       | print_info: n_ctx_orig_yarn  = 262144
llama-1       | print_info: rope_finetuned   = unknown
llama-1       | print_info: model type       = 4B
llama-1       | print_info: model params     = 4.02 B
llama-1       | print_info: general.name     = Qwen3-4B-Instruct-2507
blazor-app-1  |       End processing HTTP request after 34.5345ms - 200
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
llama-1       | print_info: vocab type       = BPE
llama-1       | print_info: n_vocab          = 151936
llama-1       | print_info: n_merges         = 151387
llama-1       | print_info: BOS token        = 11 ','
llama-1       | print_info: EOS token        = 151645 '<|im_end|>'
llama-1       | print_info: EOT token        = 151645 '<|im_end|>'
llama-1       | print_info: PAD token        = 151654 '<|vision_pad|>'
llama-1       | print_info: LF token         = 198 'Ċ'
llama-1       | print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
llama-1       | print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
llama-1       | print_info: FIM MID token    = 151660 '<|fim_middle|>'
llama-1       | print_info: FIM PAD token    = 151662 '<|fim_pad|>'
llama-1       | print_info: FIM REP token    = 151663 '<|repo_name|>'
llama-1       | print_info: FIM SEP token    = 151664 '<|file_sep|>'
llama-1       | print_info: EOG token        = 151643 '<|endoftext|>'
llama-1       | print_info: EOG token        = 151645 '<|im_end|>'
llama-1       | print_info: EOG token        = 151662 '<|fim_pad|>'
llama-1       | print_info: EOG token        = 151663 '<|repo_name|>'
llama-1       | print_info: EOG token        = 151664 '<|file_sep|>'
llama-1       | print_info: max token length = 256
llama-1       | load_tensors: loading model tensors, this can take a while... (mmap = true)
llama-1       | load_tensors:   CPU_REPACK model buffer size =  1890.00 MiB
llama-1       | load_tensors:   CPU_Mapped model buffer size =  2253.86 MiB
llama-1       | ........................................................................................
llama-1       | llama_context: constructing llama_context
llama-1       | llama_context: n_seq_max     = 1
llama-1       | llama_context: n_ctx         = 4096
llama-1       | llama_context: n_ctx_per_seq = 4096
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
llama-1       | llama_context: n_batch       = 2048
llama-1       | llama_context: n_ubatch      = 512
llama-1       | llama_context: causal_attn   = 1
llama-1       | llama_context: flash_attn    = 0
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
searxng-1     | 2025-10-07 10:26:17,014 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:26:17,021 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | 2025-10-07 10:26:32,495 ERROR:searx.botdetection: X-Forwarded-For nor X-Real-IP header is set!
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 29.5918ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 38.1995ms - 200
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:27:27,563 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:27:27,696 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:27:27,701 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 1.9935ms - 200
searxng-1     | 2025-10-07 10:27:27,702 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
llama-1       | llama_context: kv_unified    = false
llama-1       | llama_context: freq_base     = 5000000.0
llama-1       | llama_context: freq_scale    = 1
llama-1       | llama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
llama-1       | llama_context:        CPU  output buffer size =     0.58 MiB
llama-1       | llama_kv_cache:        CPU KV buffer size =   576.00 MiB
llama-1       | llama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama-1       | llama_context:        CPU compute buffer size =   301.75 MiB
llama-1       | llama_context: graph nodes  = 1410
llama-1       | llama_context: graph splits = 1
llama-1       | common_init_from_params: added <|endoftext|> logit bias = -inf
llama-1       | common_init_from_params: added <|im_end|> logit bias = -inf
llama-1       | common_init_from_params: added <|fim_pad|> logit bias = -inf
llama-1       | common_init_from_params: added <|repo_name|> logit bias = -inf
llama-1       | common_init_from_params: added <|file_sep|> logit bias = -inf
llama-1       | common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
blazor-app-1  |       End processing HTTP request after 2.1202ms - 200
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
blazor-app-1  | warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
blazor-app-1  |       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed. For more information go to https://aka.ms/aspnet/dataprotectionwarning
blazor-app-1  | info: Microsoft.Hosting.Lifetime[14]
blazor-app-1  |       Now listening on: http://[::]:8080
llama-1       | common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
searxng-1     | 2025-10-07 10:27:27,703 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application started. Press Ctrl+C to shut down.
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Hosting environment: Production
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Content root path: /app
blazor-app-1  | warn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]
blazor-app-1  |       Failed to determine the https port for redirect.
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 27.3665ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 36.2876ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[100]
blazor-app-1  |       Start processing HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[100]
blazor-app-1  |       Sending HTTP request POST http://host.docker.internal:8000/agent-chat
blazor-app-1  | info: System.Net.Http.HttpClient.Default.ClientHandler[101]
blazor-app-1  |       Received HTTP response headers after 1.8731ms - 200
blazor-app-1  | info: System.Net.Http.HttpClient.Default.LogicalHandler[101]
blazor-app-1  |       End processing HTTP request after 1.9833ms - 200
blazor-app-1  | info: Microsoft.Hosting.Lifetime[0]
blazor-app-1  |       Application is shutting down...
llama-1       | srv          init: initializing slots, n_slots = 1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:28:10,114 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:28:10,137 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:28:10,143 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:28:10,145 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
llama-1       | slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
llama-1       | main: model loaded
llama-1       | main: chat template, chat_template: {%- if tools %}
llama-1       |     {{- '<|im_start|>system\n' }}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- messages[0].content + '\n\n' }}
llama-1       |     {%- endif %}
llama-1       |     {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
llama-1       |     {%- for tool in tools %}
llama-1       |         {{- "\n" }}
llama-1       |         {{- tool | tojson }}
searxng-1     | 2025-10-07 10:28:10,147 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:31:40,814 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:31:40,842 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:31:40,850 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:31:40,853 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:31:40,855 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:33:35,192 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:33:35,218 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:33:35,226 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:33:35,228 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
llama-1       |     {%- endfor %}
llama-1       |     {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
llama-1       | {%- else %}
llama-1       |     {%- if messages[0].role == 'system' %}
searxng-1     | 2025-10-07 10:33:35,230 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:36:11,682 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:36:11,706 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
llama-1       |         {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
llama-1       |     {%- endif %}
llama-1       | {%- endif %}
llama-1       | {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
llama-1       | {%- for message in messages[::-1] %}
llama-1       |     {%- set index = (messages|length - 1) - loop.index0 %}
llama-1       |     {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
llama-1       |         {%- set ns.multi_step_tool = false %}
llama-1       |         {%- set ns.last_query_index = index %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- for message in messages %}
llama-1       |     {%- if message.content is string %}
llama-1       |         {%- set content = message.content %}
llama-1       |     {%- else %}
llama-1       |         {%- set content = '' %}
llama-1       |     {%- endif %}
searxng-1     | 2025-10-07 10:36:11,713 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:36:11,715 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:36:11,717 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
llama-1       |     {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
llama-1       |         {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
llama-1       |     {%- elif message.role == "assistant" %}
llama-1       |         {%- set reasoning_content = '' %}
llama-1       |         {%- if message.reasoning_content is string %}
llama-1       |             {%- set reasoning_content = message.reasoning_content %}
llama-1       |         {%- else %}
llama-1       |             {%- if '</think>' in content %}
llama-1       |                 {%- set reasoning_content = ((content.split('</think>')|first).rstrip('\n').split('<think>')|last).lstrip('\n') %}
llama-1       |                 {%- set content = (content.split('</think>')|last).lstrip('\n') %}
llama-1       |             {%- endif %}
llama-1       |         {%- endif %}
llama-1       |         {%- if loop.index0 > ns.last_query_index %}
llama-1       |             {%- if loop.last or (not loop.last and reasoning_content) %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
llama-1       |             {%- else %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |             {%- endif %}
llama-1       |         {%- else %}
llama-1       |             {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |         {%- endif %}
llama-1       |         {%- if message.tool_calls %}
llama-1       |             {%- for tool_call in message.tool_calls %}
llama-1       |                 {%- if (loop.first and content) or (not loop.first) %}
llama-1       |                     {{- '\n' }}
llama-1       |                 {%- endif %}
llama-1       |                 {%- if tool_call.function %}
llama-1       |                     {%- set tool_call = tool_call.function %}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '<tool_call>\n{"name": "' }}
llama-1       |                 {{- tool_call.name }}
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 14
searxng-1     | 2025-10-07 10:37:33,315 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:37:33,343 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:37:33,349 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:37:33,351 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:37:33,353 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:38:38,810 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:38:38,828 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:38:38,834 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:38:38,836 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:38:38,837 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
llama-1       |                 {{- '", "arguments": ' }}
llama-1       |                 {%- if tool_call.arguments is string %}
llama-1       |                     {{- tool_call.arguments }}
llama-1       |                 {%- else %}
llama-1       |                     {{- tool_call.arguments | tojson }}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '}\n</tool_call>' }}
llama-1       |             {%- endfor %}
llama-1       |         {%- endif %}
llama-1       |         {{- '<|im_end|>\n' }}
llama-1       |     {%- elif message.role == "tool" %}
llama-1       |         {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
llama-1       |             {{- '<|im_start|>user' }}
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:39:01,843 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:39:01,866 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:39:01,874 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:39:01,877 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:39:01,879 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | 2025-10-07 10:40:41,104 ERROR:searx.botdetection: X-Forwarded-For nor X-Real-IP header is set!
searxng-1     | /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=15) is multi-threaded, use of fork() may lead to deadlocks in the child.
searxng-1     |   self.pid = os.fork()
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:45:07,743 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:45:07,765 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:45:07,773 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:45:07,776 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:45:07,778 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:45:59,805 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:45:59,831 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:45:59,838 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
llama-1       |         {%- endif %}
llama-1       |         {{- '\n<tool_response>\n' }}
llama-1       |         {{- content }}
llama-1       |         {{- '\n</tool_response>' }}
llama-1       |         {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
llama-1       |             {{- '<|im_end|>\n' }}
llama-1       |         {%- endif %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- if add_generation_prompt %}
llama-1       |     {{- '<|im_start|>assistant\n' }}
llama-1       | {%- endif %}, example_format: '<|im_start|>system
llama-1       | You are a helpful assistant<|im_end|>
llama-1       | <|im_start|>user
llama-1       | Hello<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | Hi there<|im_end|>
llama-1       | <|im_start|>user
llama-1       | How are you?<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | '
llama-1       | main: server is listening on http://0.0.0.0:8080 - starting the main loop
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 0 | processing task
llama-1       | slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 60
llama-1       | slot update_slots: id  0 | task 0 | kv cache rm [0, end)
llama-1       | slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 60, n_tokens = 60, progress = 1.000000
llama-1       | slot update_slots: id  0 | task 0 | prompt done, n_past = 60, n_tokens = 60
llama-1       | slot      release: id  0 | task 0 | stop processing: n_past = 81, truncated = 0
llama-1       | slot print_timing: id  0 | task 0 | 
llama-1       | prompt eval time =    1125.47 ms /    60 tokens (   18.76 ms per token,    53.31 tokens per second)
llama-1       |        eval time =    1989.78 ms /    22 tokens (   90.44 ms per token,    11.06 tokens per second)
llama-1       |       total time =    3115.25 ms /    82 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.1 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 23 | processing task
llama-1       | slot update_slots: id  0 | task 23 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 526
llama-1       | slot update_slots: id  0 | task 23 | kv cache rm [1, end)
llama-1       | slot update_slots: id  0 | task 23 | prompt processing progress, n_past = 526, n_tokens = 525, progress = 0.998099
llama-1       | slot update_slots: id  0 | task 23 | prompt done, n_past = 526, n_tokens = 525
llama-1       | slot      release: id  0 | task 23 | stop processing: n_past = 537, truncated = 0
llama-1       | slot print_timing: id  0 | task 23 | 
llama-1       | prompt eval time =    8497.24 ms /   525 tokens (   16.19 ms per token,    61.78 tokens per second)
llama-1       |        eval time =    1032.26 ms /    12 tokens (   86.02 ms per token,    11.62 tokens per second)
llama-1       |       total time =    9529.50 ms /   537 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 36 | processing task
llama-1       | slot update_slots: id  0 | task 36 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 534
llama-1       | slot update_slots: id  0 | task 36 | kv cache rm [520, end)
llama-1       | slot update_slots: id  0 | task 36 | prompt processing progress, n_past = 534, n_tokens = 14, progress = 0.026217
llama-1       | slot update_slots: id  0 | task 36 | prompt done, n_past = 534, n_tokens = 14
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | slot      release: id  0 | task 36 | stop processing: n_past = 560, truncated = 0
llama-1       | slot print_timing: id  0 | task 36 | 
llama-1       | prompt eval time =     332.41 ms /    14 tokens (   23.74 ms per token,    42.12 tokens per second)
llama-1       |        eval time =    2493.70 ms /    27 tokens (   92.36 ms per token,    10.83 tokens per second)
llama-1       |       total time =    2826.10 ms /    41 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 64 | processing task
llama-1       | slot update_slots: id  0 | task 64 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 672
llama-1       | slot update_slots: id  0 | task 64 | kv cache rm [560, end)
llama-1       | slot update_slots: id  0 | task 64 | prompt processing progress, n_past = 672, n_tokens = 112, progress = 0.166667
searxng-1     | 2025-10-07 10:45:59,841 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:45:59,842 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:46:44,971 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:46:44,999 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
llama-1       | slot update_slots: id  0 | task 64 | prompt done, n_past = 672, n_tokens = 112
llama-1       | slot      release: id  0 | task 64 | stop processing: n_past = 696, truncated = 0
searxng-1     | 2025-10-07 10:46:45,008 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:46:45,011 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:46:45,012 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | [INFO] Stopping worker-1
searxng-1     | SearXNG 2025.10.1-06e4f4f75
searxng-1     | chown: /etc/searxng/settings.yml: Read-only file system
searxng-1     | [INFO] Starting granian (main PID: 1)
searxng-1     | [INFO] Listening at: http://:::8080
searxng-1     | [INFO] Spawning worker-1 with PID: 15
searxng-1     | 2025-10-07 10:56:19,230 ERROR:searx.engines: Missing engine config attribute: "yacy images.base_url"
searxng-1     | 2025-10-07 10:56:19,253 WARNING:searx.search.processors: Engine of name 'ahmia' does not exists.
searxng-1     | 2025-10-07 10:56:19,262 WARNING:searx.search.processors: Engine of name 'torch' does not exists.
searxng-1     | 2025-10-07 10:56:19,264 WARNING:searx.search.processors: Engine of name 'yacy images' does not exists.
searxng-1     | 2025-10-07 10:56:19,266 WARNING:searx.botdetection.config: missing config file: /etc/searxng/limiter.toml
searxng-1     | [INFO] Started worker-1
searxng-1     | [INFO] Started worker-1 runtime-1
searxng-1     | [INFO] Shutting down granian
searxng-1     | [INFO] Stopping worker-1 runtime-1
searxng-1     | [INFO] Stopping worker-1
llama-1       | slot print_timing: id  0 | task 64 | 
llama-1       | prompt eval time =    1921.97 ms /   112 tokens (   17.16 ms per token,    58.27 tokens per second)
llama-1       |        eval time =    2314.85 ms /    25 tokens (   92.59 ms per token,    10.80 tokens per second)
llama-1       |       total time =    4236.82 ms /   137 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 90 | processing task
llama-1       | slot update_slots: id  0 | task 90 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 530
llama-1       | slot update_slots: id  0 | task 90 | kv cache rm [522, end)
llama-1       | slot update_slots: id  0 | task 90 | prompt processing progress, n_past = 530, n_tokens = 8, progress = 0.015094
llama-1       | slot update_slots: id  0 | task 90 | prompt done, n_past = 530, n_tokens = 8
llama-1       | slot      release: id  0 | task 90 | stop processing: n_past = 565, truncated = 0
llama-1       | slot print_timing: id  0 | task 90 | 
llama-1       | prompt eval time =     203.32 ms /     8 tokens (   25.41 ms per token,    39.35 tokens per second)
llama-1       |        eval time =    3267.25 ms /    36 tokens (   90.76 ms per token,    11.02 tokens per second)
llama-1       |       total time =    3470.57 ms /    44 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 127 | processing task
llama-1       | slot update_slots: id  0 | task 127 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 534
llama-1       | slot update_slots: id  0 | task 127 | kv cache rm [520, end)
llama-1       | slot update_slots: id  0 | task 127 | prompt processing progress, n_past = 534, n_tokens = 14, progress = 0.026217
llama-1       | slot update_slots: id  0 | task 127 | prompt done, n_past = 534, n_tokens = 14
llama-1       | slot      release: id  0 | task 127 | stop processing: n_past = 560, truncated = 0
llama-1       | slot print_timing: id  0 | task 127 | 
llama-1       | prompt eval time =     359.13 ms /    14 tokens (   25.65 ms per token,    38.98 tokens per second)
llama-1       |        eval time =    2477.95 ms /    27 tokens (   91.78 ms per token,    10.90 tokens per second)
llama-1       |       total time =    2837.08 ms /    41 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 155 | processing task
llama-1       | slot update_slots: id  0 | task 155 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 673
llama-1       | slot update_slots: id  0 | task 155 | kv cache rm [560, end)
llama-1       | slot update_slots: id  0 | task 155 | prompt processing progress, n_past = 673, n_tokens = 113, progress = 0.167905
llama-1       | slot update_slots: id  0 | task 155 | prompt done, n_past = 673, n_tokens = 113
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | slot      release: id  0 | task 155 | stop processing: n_past = 698, truncated = 0
llama-1       | slot print_timing: id  0 | task 155 | 
llama-1       | prompt eval time =    1908.38 ms /   113 tokens (   16.89 ms per token,    59.21 tokens per second)
llama-1       |        eval time =    2436.77 ms /    26 tokens (   93.72 ms per token,    10.67 tokens per second)
llama-1       |       total time =    4345.15 ms /   139 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 182 | processing task
llama-1       | slot update_slots: id  0 | task 182 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 528
llama-1       | slot update_slots: id  0 | task 182 | kv cache rm [520, end)
llama-1       | slot update_slots: id  0 | task 182 | prompt processing progress, n_past = 528, n_tokens = 8, progress = 0.015152
llama-1       | slot update_slots: id  0 | task 182 | prompt done, n_past = 528, n_tokens = 8
llama-1       | slot      release: id  0 | task 182 | stop processing: n_past = 548, truncated = 0
llama-1       | slot print_timing: id  0 | task 182 | 
llama-1       | prompt eval time =     227.75 ms /     8 tokens (   28.47 ms per token,    35.13 tokens per second)
llama-1       |        eval time =    1928.37 ms /    21 tokens (   91.83 ms per token,    10.89 tokens per second)
llama-1       |       total time =    2156.12 ms /    29 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 204 | processing task
llama-1       | slot update_slots: id  0 | task 204 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 561
llama-1       | slot update_slots: id  0 | task 204 | kv cache rm [548, end)
llama-1       | slot update_slots: id  0 | task 204 | prompt processing progress, n_past = 561, n_tokens = 13, progress = 0.023173
llama-1       | slot update_slots: id  0 | task 204 | prompt done, n_past = 561, n_tokens = 13
llama-1       | slot      release: id  0 | task 204 | stop processing: n_past = 592, truncated = 0
llama-1       | slot print_timing: id  0 | task 204 | 
llama-1       | prompt eval time =     309.40 ms /    13 tokens (   23.80 ms per token,    42.02 tokens per second)
llama-1       |        eval time =    2968.58 ms /    32 tokens (   92.77 ms per token,    10.78 tokens per second)
llama-1       |       total time =    3277.98 ms /    45 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 237 | processing task
llama-1       | slot update_slots: id  0 | task 237 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 528
llama-1       | slot update_slots: id  0 | task 237 | need to evaluate at least 1 token for each active slot, n_past = 528, n_prompt_tokens = 528
llama-1       | slot update_slots: id  0 | task 237 | kv cache rm [527, end)
llama-1       | slot update_slots: id  0 | task 237 | prompt processing progress, n_past = 528, n_tokens = 1, progress = 0.001894
llama-1       | slot update_slots: id  0 | task 237 | prompt done, n_past = 528, n_tokens = 1
llama-1       | slot      release: id  0 | task 237 | stop processing: n_past = 548, truncated = 0
llama-1       | slot print_timing: id  0 | task 237 | 
llama-1       | prompt eval time =     103.39 ms /     1 tokens (  103.39 ms per token,     9.67 tokens per second)
llama-1       |        eval time =    1907.68 ms /    21 tokens (   90.84 ms per token,    11.01 tokens per second)
llama-1       |       total time =    2011.07 ms /    22 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 259 | processing task
llama-1       | slot update_slots: id  0 | task 259 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 561
llama-1       | slot update_slots: id  0 | task 259 | kv cache rm [548, end)
llama-1       | slot update_slots: id  0 | task 259 | prompt processing progress, n_past = 561, n_tokens = 13, progress = 0.023173
llama-1       | slot update_slots: id  0 | task 259 | prompt done, n_past = 561, n_tokens = 13
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | slot      release: id  0 | task 259 | stop processing: n_past = 592, truncated = 0
llama-1       | slot print_timing: id  0 | task 259 | 
llama-1       | prompt eval time =     296.80 ms /    13 tokens (   22.83 ms per token,    43.80 tokens per second)
llama-1       |        eval time =    3014.23 ms /    32 tokens (   94.19 ms per token,    10.62 tokens per second)
llama-1       |       total time =    3311.02 ms /    45 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.5 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv    operator(): operator(): cleaning up before exit...
llama-1       | ggml_vulkan: No devices found.
llama-1       | load_backend: loaded Vulkan backend from /app/libggml-vulkan.so
llama-1       | load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
llama-1       | build: 6265 (c247d06f) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu
llama-1       | system info: n_threads = 6, n_threads_batch = 6, total_threads = 12
llama-1       | 
llama-1       | system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
llama-1       | 
llama-1       | main: binding port with default address family
llama-1       | main: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 11
llama-1       | main: loading model
llama-1       | srv    load_model: loading model '/models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf'
llama-1       | llama_model_loader: loaded meta data with 42 key-value pairs and 398 tensors from /models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf (version GGUF V3 (latest))
llama-1       | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama-1       | llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama-1       | llama_model_loader: - kv   1:                               general.type str              = model
llama-1       | llama_model_loader: - kv   2:                               general.name str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   3:                            general.version str              = 2507
llama-1       | llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama-1       | llama_model_loader: - kv   5:                           general.basename str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth
llama-1       | llama_model_loader: - kv   7:                         general.size_label str              = 4B
llama-1       | llama_model_loader: - kv   8:                            general.license str              = apache-2.0
llama-1       | llama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  10:                           general.repo_url str              = https://huggingface.co/unsloth
llama-1       | llama_model_loader: - kv  11:                   general.base_model.count u32              = 1
llama-1       | llama_model_loader: - kv  12:                  general.base_model.0.name str              = Qwen3 4B Instruct 2507
llama-1       | llama_model_loader: - kv  13:               general.base_model.0.version str              = 2507
llama-1       | llama_model_loader: - kv  14:          general.base_model.0.organization str              = Qwen
llama-1       | llama_model_loader: - kv  15:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  16:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
llama-1       | llama_model_loader: - kv  17:                          qwen3.block_count u32              = 36
llama-1       | llama_model_loader: - kv  18:                       qwen3.context_length u32              = 262144
llama-1       | llama_model_loader: - kv  19:                     qwen3.embedding_length u32              = 2560
llama-1       | llama_model_loader: - kv  20:                  qwen3.feed_forward_length u32              = 9728
llama-1       | llama_model_loader: - kv  21:                 qwen3.attention.head_count u32              = 32
llama-1       | llama_model_loader: - kv  22:              qwen3.attention.head_count_kv u32              = 8
llama-1       | llama_model_loader: - kv  23:                       qwen3.rope.freq_base f32              = 5000000.000000
llama-1       | llama_model_loader: - kv  24:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama-1       | llama_model_loader: - kv  25:                 qwen3.attention.key_length u32              = 128
llama-1       | llama_model_loader: - kv  26:               qwen3.attention.value_length u32              = 128
llama-1       | llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama-1       | llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama-1       | llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama-1       | llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama-1       | llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama-1       | llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645
llama-1       | llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151654
llama-1       | llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false
llama-1       | llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama-1       | llama_model_loader: - kv  36:               general.quantization_version u32              = 2
llama-1       | llama_model_loader: - kv  37:                          general.file_type u32              = 14
llama-1       | llama_model_loader: - kv  38:                      quantize.imatrix.file str              = Qwen3-4B-Instruct-2507-GGUF/imatrix_u...
llama-1       | llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-4B-Instruct...
llama-1       | llama_model_loader: - kv  40:             quantize.imatrix.entries_count u32              = 252
llama-1       | llama_model_loader: - kv  41:              quantize.imatrix.chunks_count u32              = 79
llama-1       | llama_model_loader: - type  f32:  145 tensors
llama-1       | llama_model_loader: - type q4_K:  244 tensors
llama-1       | llama_model_loader: - type q5_K:    8 tensors
llama-1       | llama_model_loader: - type q6_K:    1 tensors
llama-1       | print_info: file format = GGUF V3 (latest)
llama-1       | print_info: file type   = Q4_K - Small
llama-1       | print_info: file size   = 2.21 GiB (4.73 BPW) 
llama-1       | load: printing all EOG tokens:
llama-1       | load:   - 151643 ('<|endoftext|>')
llama-1       | load:   - 151645 ('<|im_end|>')
llama-1       | load:   - 151662 ('<|fim_pad|>')
llama-1       | load:   - 151663 ('<|repo_name|>')
llama-1       | load:   - 151664 ('<|file_sep|>')
llama-1       | load: special tokens cache size = 26
llama-1       | load: token to piece cache size = 0.9311 MB
llama-1       | print_info: arch             = qwen3
llama-1       | print_info: vocab_only       = 0
llama-1       | print_info: n_ctx_train      = 262144
llama-1       | print_info: n_embd           = 2560
llama-1       | print_info: n_layer          = 36
llama-1       | print_info: n_head           = 32
llama-1       | print_info: n_head_kv        = 8
llama-1       | print_info: n_rot            = 128
llama-1       | print_info: n_swa            = 0
llama-1       | print_info: is_swa_any       = 0
llama-1       | print_info: n_embd_head_k    = 128
llama-1       | print_info: n_embd_head_v    = 128
llama-1       | print_info: n_gqa            = 4
llama-1       | print_info: n_embd_k_gqa     = 1024
llama-1       | print_info: n_embd_v_gqa     = 1024
llama-1       | print_info: f_norm_eps       = 0.0e+00
llama-1       | print_info: f_norm_rms_eps   = 1.0e-06
llama-1       | print_info: f_clamp_kqv      = 0.0e+00
llama-1       | print_info: f_max_alibi_bias = 0.0e+00
llama-1       | print_info: f_logit_scale    = 0.0e+00
llama-1       | print_info: f_attn_scale     = 0.0e+00
llama-1       | print_info: n_ff             = 9728
llama-1       | print_info: n_expert         = 0
llama-1       | print_info: n_expert_used    = 0
llama-1       | print_info: causal attn      = 1
llama-1       | print_info: pooling type     = -1
llama-1       | print_info: rope type        = 2
llama-1       | print_info: rope scaling     = linear
llama-1       | print_info: freq_base_train  = 5000000.0
llama-1       | print_info: freq_scale_train = 1
llama-1       | print_info: n_ctx_orig_yarn  = 262144
llama-1       | print_info: rope_finetuned   = unknown
llama-1       | print_info: model type       = 4B
llama-1       | print_info: model params     = 4.02 B
llama-1       | print_info: general.name     = Qwen3-4B-Instruct-2507
llama-1       | print_info: vocab type       = BPE
llama-1       | print_info: n_vocab          = 151936
llama-1       | print_info: n_merges         = 151387
llama-1       | print_info: BOS token        = 11 ','
llama-1       | print_info: EOS token        = 151645 '<|im_end|>'
llama-1       | print_info: EOT token        = 151645 '<|im_end|>'
llama-1       | print_info: PAD token        = 151654 '<|vision_pad|>'
llama-1       | print_info: LF token         = 198 'Ċ'
llama-1       | print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
llama-1       | print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
llama-1       | print_info: FIM MID token    = 151660 '<|fim_middle|>'
llama-1       | print_info: FIM PAD token    = 151662 '<|fim_pad|>'
llama-1       | print_info: FIM REP token    = 151663 '<|repo_name|>'
llama-1       | print_info: FIM SEP token    = 151664 '<|file_sep|>'
llama-1       | print_info: EOG token        = 151643 '<|endoftext|>'
llama-1       | print_info: EOG token        = 151645 '<|im_end|>'
llama-1       | print_info: EOG token        = 151662 '<|fim_pad|>'
llama-1       | print_info: EOG token        = 151663 '<|repo_name|>'
llama-1       | print_info: EOG token        = 151664 '<|file_sep|>'
llama-1       | print_info: max token length = 256
llama-1       | load_tensors: loading model tensors, this can take a while... (mmap = true)
llama-1       | load_tensors:   CPU_REPACK model buffer size =  1890.00 MiB
llama-1       | load_tensors:   CPU_Mapped model buffer size =  2253.86 MiB
llama-1       | ........................................................................................
llama-1       | llama_context: constructing llama_context
llama-1       | llama_context: n_seq_max     = 1
llama-1       | llama_context: n_ctx         = 4096
llama-1       | llama_context: n_ctx_per_seq = 4096
llama-1       | llama_context: n_batch       = 2048
llama-1       | llama_context: n_ubatch      = 512
llama-1       | llama_context: causal_attn   = 1
llama-1       | llama_context: flash_attn    = 0
llama-1       | llama_context: kv_unified    = false
llama-1       | llama_context: freq_base     = 5000000.0
llama-1       | llama_context: freq_scale    = 1
llama-1       | llama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
llama-1       | llama_context:        CPU  output buffer size =     0.58 MiB
llama-1       | llama_kv_cache:        CPU KV buffer size =   576.00 MiB
llama-1       | llama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama-1       | llama_context:        CPU compute buffer size =   301.75 MiB
llama-1       | llama_context: graph nodes  = 1410
llama-1       | llama_context: graph splits = 1
llama-1       | common_init_from_params: added <|endoftext|> logit bias = -inf
llama-1       | common_init_from_params: added <|im_end|> logit bias = -inf
llama-1       | common_init_from_params: added <|fim_pad|> logit bias = -inf
llama-1       | common_init_from_params: added <|repo_name|> logit bias = -inf
llama-1       | common_init_from_params: added <|file_sep|> logit bias = -inf
llama-1       | common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
llama-1       | common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
llama-1       | srv          init: initializing slots, n_slots = 1
llama-1       | slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
llama-1       | main: model loaded
llama-1       | main: chat template, chat_template: {%- if tools %}
llama-1       |     {{- '<|im_start|>system\n' }}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- messages[0].content + '\n\n' }}
llama-1       |     {%- endif %}
llama-1       |     {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
llama-1       |     {%- for tool in tools %}
llama-1       |         {{- "\n" }}
llama-1       |         {{- tool | tojson }}
llama-1       |     {%- endfor %}
llama-1       |     {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
llama-1       | {%- else %}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
llama-1       |     {%- endif %}
llama-1       | {%- endif %}
llama-1       | {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
llama-1       | {%- for message in messages[::-1] %}
llama-1       |     {%- set index = (messages|length - 1) - loop.index0 %}
llama-1       |     {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
llama-1       |         {%- set ns.multi_step_tool = false %}
llama-1       |         {%- set ns.last_query_index = index %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- for message in messages %}
llama-1       |     {%- if message.content is string %}
llama-1       |         {%- set content = message.content %}
llama-1       |     {%- else %}
llama-1       |         {%- set content = '' %}
llama-1       |     {%- endif %}
llama-1       |     {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
llama-1       |         {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
llama-1       |     {%- elif message.role == "assistant" %}
llama-1       |         {%- set reasoning_content = '' %}
llama-1       |         {%- if message.reasoning_content is string %}
llama-1       |             {%- set reasoning_content = message.reasoning_content %}
llama-1       |         {%- else %}
llama-1       |             {%- if '</think>' in content %}
llama-1       |                 {%- set reasoning_content = ((content.split('</think>')|first).rstrip('\n').split('<think>')|last).lstrip('\n') %}
llama-1       |                 {%- set content = (content.split('</think>')|last).lstrip('\n') %}
llama-1       |             {%- endif %}
llama-1       |         {%- endif %}
llama-1       |         {%- if loop.index0 > ns.last_query_index %}
llama-1       |             {%- if loop.last or (not loop.last and reasoning_content) %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
llama-1       |             {%- else %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |             {%- endif %}
llama-1       |         {%- else %}
llama-1       |             {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |         {%- endif %}
llama-1       |         {%- if message.tool_calls %}
llama-1       |             {%- for tool_call in message.tool_calls %}
llama-1       |                 {%- if (loop.first and content) or (not loop.first) %}
llama-1       |                     {{- '\n' }}
llama-1       |                 {%- endif %}
llama-1       |                 {%- if tool_call.function %}
llama-1       |                     {%- set tool_call = tool_call.function %}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '<tool_call>\n{"name": "' }}
llama-1       |                 {{- tool_call.name }}
llama-1       |                 {{- '", "arguments": ' }}
llama-1       |                 {%- if tool_call.arguments is string %}
llama-1       |                     {{- tool_call.arguments }}
llama-1       |                 {%- else %}
llama-1       |                     {{- tool_call.arguments | tojson }}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '}\n</tool_call>' }}
llama-1       |             {%- endfor %}
llama-1       |         {%- endif %}
llama-1       |         {{- '<|im_end|>\n' }}
llama-1       |     {%- elif message.role == "tool" %}
llama-1       |         {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
llama-1       |             {{- '<|im_start|>user' }}
llama-1       |         {%- endif %}
llama-1       |         {{- '\n<tool_response>\n' }}
llama-1       |         {{- content }}
llama-1       |         {{- '\n</tool_response>' }}
llama-1       |         {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
llama-1       |             {{- '<|im_end|>\n' }}
llama-1       |         {%- endif %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- if add_generation_prompt %}
llama-1       |     {{- '<|im_start|>assistant\n' }}
llama-1       | {%- endif %}, example_format: '<|im_start|>system
llama-1       | You are a helpful assistant<|im_end|>
llama-1       | <|im_start|>user
llama-1       | Hello<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | Hi there<|im_end|>
llama-1       | <|im_start|>user
llama-1       | How are you?<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | '
llama-1       | main: server is listening on http://0.0.0.0:8080 - starting the main loop
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 0 | processing task
llama-1       | slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 526
llama-1       | slot update_slots: id  0 | task 0 | kv cache rm [0, end)
llama-1       | slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 526, n_tokens = 526, progress = 1.000000
llama-1       | slot update_slots: id  0 | task 0 | prompt done, n_past = 526, n_tokens = 526
llama-1       | slot      release: id  0 | task 0 | stop processing: n_past = 537, truncated = 0
llama-1       | slot print_timing: id  0 | task 0 | 
llama-1       | prompt eval time =    8678.50 ms /   526 tokens (   16.50 ms per token,    60.61 tokens per second)
llama-1       |        eval time =    1045.14 ms /    12 tokens (   87.10 ms per token,    11.48 tokens per second)
llama-1       |       total time =    9723.64 ms /   538 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.3 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 13 | processing task
llama-1       | slot update_slots: id  0 | task 13 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 528
llama-1       | slot update_slots: id  0 | task 13 | kv cache rm [520, end)
llama-1       | slot update_slots: id  0 | task 13 | prompt processing progress, n_past = 528, n_tokens = 8, progress = 0.015152
llama-1       | slot update_slots: id  0 | task 13 | prompt done, n_past = 528, n_tokens = 8
llama-1       | slot      release: id  0 | task 13 | stop processing: n_past = 548, truncated = 0
llama-1       | slot print_timing: id  0 | task 13 | 
llama-1       | prompt eval time =     206.26 ms /     8 tokens (   25.78 ms per token,    38.79 tokens per second)
llama-1       |        eval time =    1846.98 ms /    21 tokens (   87.95 ms per token,    11.37 tokens per second)
llama-1       |       total time =    2053.24 ms /    29 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.3 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 35 | processing task
llama-1       | slot update_slots: id  0 | task 35 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 561
llama-1       | slot update_slots: id  0 | task 35 | kv cache rm [548, end)
llama-1       | slot update_slots: id  0 | task 35 | prompt processing progress, n_past = 561, n_tokens = 13, progress = 0.023173
llama-1       | slot update_slots: id  0 | task 35 | prompt done, n_past = 561, n_tokens = 13
llama-1       | slot      release: id  0 | task 35 | stop processing: n_past = 592, truncated = 0
llama-1       | slot print_timing: id  0 | task 35 | 
llama-1       | prompt eval time =     330.45 ms /    13 tokens (   25.42 ms per token,    39.34 tokens per second)
llama-1       |        eval time =    2984.36 ms /    32 tokens (   93.26 ms per token,    10.72 tokens per second)
llama-1       |       total time =    3314.81 ms /    45 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.3 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv    operator(): operator(): cleaning up before exit...
llama-1       | ggml_vulkan: No devices found.
llama-1       | load_backend: loaded Vulkan backend from /app/libggml-vulkan.so
llama-1       | load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
llama-1       | build: 6265 (c247d06f) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu
llama-1       | system info: n_threads = 6, n_threads_batch = 6, total_threads = 12
llama-1       | 
llama-1       | system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
llama-1       | 
llama-1       | main: binding port with default address family
llama-1       | main: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 11
llama-1       | main: loading model
llama-1       | srv    load_model: loading model '/models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf'
llama-1       | llama_model_loader: loaded meta data with 42 key-value pairs and 398 tensors from /models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf (version GGUF V3 (latest))
llama-1       | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama-1       | llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama-1       | llama_model_loader: - kv   1:                               general.type str              = model
llama-1       | llama_model_loader: - kv   2:                               general.name str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   3:                            general.version str              = 2507
llama-1       | llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama-1       | llama_model_loader: - kv   5:                           general.basename str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth
llama-1       | llama_model_loader: - kv   7:                         general.size_label str              = 4B
llama-1       | llama_model_loader: - kv   8:                            general.license str              = apache-2.0
llama-1       | llama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  10:                           general.repo_url str              = https://huggingface.co/unsloth
llama-1       | llama_model_loader: - kv  11:                   general.base_model.count u32              = 1
llama-1       | llama_model_loader: - kv  12:                  general.base_model.0.name str              = Qwen3 4B Instruct 2507
llama-1       | llama_model_loader: - kv  13:               general.base_model.0.version str              = 2507
llama-1       | llama_model_loader: - kv  14:          general.base_model.0.organization str              = Qwen
llama-1       | llama_model_loader: - kv  15:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  16:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
llama-1       | llama_model_loader: - kv  17:                          qwen3.block_count u32              = 36
llama-1       | llama_model_loader: - kv  18:                       qwen3.context_length u32              = 262144
llama-1       | llama_model_loader: - kv  19:                     qwen3.embedding_length u32              = 2560
llama-1       | llama_model_loader: - kv  20:                  qwen3.feed_forward_length u32              = 9728
llama-1       | llama_model_loader: - kv  21:                 qwen3.attention.head_count u32              = 32
llama-1       | llama_model_loader: - kv  22:              qwen3.attention.head_count_kv u32              = 8
llama-1       | llama_model_loader: - kv  23:                       qwen3.rope.freq_base f32              = 5000000.000000
llama-1       | llama_model_loader: - kv  24:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama-1       | llama_model_loader: - kv  25:                 qwen3.attention.key_length u32              = 128
llama-1       | llama_model_loader: - kv  26:               qwen3.attention.value_length u32              = 128
llama-1       | llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama-1       | llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama-1       | llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama-1       | llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama-1       | llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama-1       | llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645
llama-1       | llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151654
llama-1       | llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false
llama-1       | llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama-1       | llama_model_loader: - kv  36:               general.quantization_version u32              = 2
llama-1       | llama_model_loader: - kv  37:                          general.file_type u32              = 14
llama-1       | llama_model_loader: - kv  38:                      quantize.imatrix.file str              = Qwen3-4B-Instruct-2507-GGUF/imatrix_u...
llama-1       | llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-4B-Instruct...
llama-1       | llama_model_loader: - kv  40:             quantize.imatrix.entries_count u32              = 252
llama-1       | llama_model_loader: - kv  41:              quantize.imatrix.chunks_count u32              = 79
llama-1       | llama_model_loader: - type  f32:  145 tensors
llama-1       | llama_model_loader: - type q4_K:  244 tensors
llama-1       | llama_model_loader: - type q5_K:    8 tensors
llama-1       | llama_model_loader: - type q6_K:    1 tensors
llama-1       | print_info: file format = GGUF V3 (latest)
llama-1       | print_info: file type   = Q4_K - Small
llama-1       | print_info: file size   = 2.21 GiB (4.73 BPW) 
llama-1       | load: printing all EOG tokens:
llama-1       | load:   - 151643 ('<|endoftext|>')
llama-1       | load:   - 151645 ('<|im_end|>')
llama-1       | load:   - 151662 ('<|fim_pad|>')
llama-1       | load:   - 151663 ('<|repo_name|>')
llama-1       | load:   - 151664 ('<|file_sep|>')
llama-1       | load: special tokens cache size = 26
llama-1       | load: token to piece cache size = 0.9311 MB
llama-1       | print_info: arch             = qwen3
llama-1       | print_info: vocab_only       = 0
llama-1       | print_info: n_ctx_train      = 262144
llama-1       | print_info: n_embd           = 2560
llama-1       | print_info: n_layer          = 36
llama-1       | print_info: n_head           = 32
llama-1       | print_info: n_head_kv        = 8
llama-1       | print_info: n_rot            = 128
llama-1       | print_info: n_swa            = 0
llama-1       | print_info: is_swa_any       = 0
llama-1       | print_info: n_embd_head_k    = 128
llama-1       | print_info: n_embd_head_v    = 128
llama-1       | print_info: n_gqa            = 4
llama-1       | print_info: n_embd_k_gqa     = 1024
llama-1       | print_info: n_embd_v_gqa     = 1024
llama-1       | print_info: f_norm_eps       = 0.0e+00
llama-1       | print_info: f_norm_rms_eps   = 1.0e-06
llama-1       | print_info: f_clamp_kqv      = 0.0e+00
llama-1       | print_info: f_max_alibi_bias = 0.0e+00
llama-1       | print_info: f_logit_scale    = 0.0e+00
llama-1       | print_info: f_attn_scale     = 0.0e+00
llama-1       | print_info: n_ff             = 9728
llama-1       | print_info: n_expert         = 0
llama-1       | print_info: n_expert_used    = 0
llama-1       | print_info: causal attn      = 1
llama-1       | print_info: pooling type     = -1
llama-1       | print_info: rope type        = 2
llama-1       | print_info: rope scaling     = linear
llama-1       | print_info: freq_base_train  = 5000000.0
llama-1       | print_info: freq_scale_train = 1
llama-1       | print_info: n_ctx_orig_yarn  = 262144
llama-1       | print_info: rope_finetuned   = unknown
llama-1       | print_info: model type       = 4B
llama-1       | print_info: model params     = 4.02 B
llama-1       | print_info: general.name     = Qwen3-4B-Instruct-2507
llama-1       | print_info: vocab type       = BPE
llama-1       | print_info: n_vocab          = 151936
llama-1       | print_info: n_merges         = 151387
llama-1       | print_info: BOS token        = 11 ','
llama-1       | print_info: EOS token        = 151645 '<|im_end|>'
llama-1       | print_info: EOT token        = 151645 '<|im_end|>'
llama-1       | print_info: PAD token        = 151654 '<|vision_pad|>'
llama-1       | print_info: LF token         = 198 'Ċ'
llama-1       | print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
llama-1       | print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
llama-1       | print_info: FIM MID token    = 151660 '<|fim_middle|>'
llama-1       | print_info: FIM PAD token    = 151662 '<|fim_pad|>'
llama-1       | print_info: FIM REP token    = 151663 '<|repo_name|>'
llama-1       | print_info: FIM SEP token    = 151664 '<|file_sep|>'
llama-1       | print_info: EOG token        = 151643 '<|endoftext|>'
llama-1       | print_info: EOG token        = 151645 '<|im_end|>'
llama-1       | print_info: EOG token        = 151662 '<|fim_pad|>'
llama-1       | print_info: EOG token        = 151663 '<|repo_name|>'
llama-1       | print_info: EOG token        = 151664 '<|file_sep|>'
llama-1       | print_info: max token length = 256
llama-1       | load_tensors: loading model tensors, this can take a while... (mmap = true)
llama-1       | load_tensors:   CPU_REPACK model buffer size =  1890.00 MiB
llama-1       | load_tensors:   CPU_Mapped model buffer size =  2253.86 MiB
llama-1       | ........................................................................................
llama-1       | llama_context: constructing llama_context
llama-1       | llama_context: n_seq_max     = 1
llama-1       | llama_context: n_ctx         = 4096
llama-1       | llama_context: n_ctx_per_seq = 4096
llama-1       | llama_context: n_batch       = 2048
llama-1       | llama_context: n_ubatch      = 512
llama-1       | llama_context: causal_attn   = 1
llama-1       | llama_context: flash_attn    = 0
llama-1       | llama_context: kv_unified    = false
llama-1       | llama_context: freq_base     = 5000000.0
llama-1       | llama_context: freq_scale    = 1
llama-1       | llama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
llama-1       | llama_context:        CPU  output buffer size =     0.58 MiB
llama-1       | llama_kv_cache:        CPU KV buffer size =   576.00 MiB
llama-1       | llama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama-1       | llama_context:        CPU compute buffer size =   301.75 MiB
llama-1       | llama_context: graph nodes  = 1410
llama-1       | llama_context: graph splits = 1
llama-1       | common_init_from_params: added <|endoftext|> logit bias = -inf
llama-1       | common_init_from_params: added <|im_end|> logit bias = -inf
llama-1       | common_init_from_params: added <|fim_pad|> logit bias = -inf
llama-1       | common_init_from_params: added <|repo_name|> logit bias = -inf
llama-1       | common_init_from_params: added <|file_sep|> logit bias = -inf
llama-1       | common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
llama-1       | common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
llama-1       | srv          init: initializing slots, n_slots = 1
llama-1       | slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
llama-1       | main: model loaded
llama-1       | main: chat template, chat_template: {%- if tools %}
llama-1       |     {{- '<|im_start|>system\n' }}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- messages[0].content + '\n\n' }}
llama-1       |     {%- endif %}
llama-1       |     {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
llama-1       |     {%- for tool in tools %}
llama-1       |         {{- "\n" }}
llama-1       |         {{- tool | tojson }}
llama-1       |     {%- endfor %}
llama-1       |     {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
llama-1       | {%- else %}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
llama-1       |     {%- endif %}
llama-1       | {%- endif %}
llama-1       | {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
llama-1       | {%- for message in messages[::-1] %}
llama-1       |     {%- set index = (messages|length - 1) - loop.index0 %}
llama-1       |     {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
llama-1       |         {%- set ns.multi_step_tool = false %}
llama-1       |         {%- set ns.last_query_index = index %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- for message in messages %}
llama-1       |     {%- if message.content is string %}
llama-1       |         {%- set content = message.content %}
llama-1       |     {%- else %}
llama-1       |         {%- set content = '' %}
llama-1       |     {%- endif %}
llama-1       |     {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
llama-1       |         {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
llama-1       |     {%- elif message.role == "assistant" %}
llama-1       |         {%- set reasoning_content = '' %}
llama-1       |         {%- if message.reasoning_content is string %}
llama-1       |             {%- set reasoning_content = message.reasoning_content %}
llama-1       |         {%- else %}
llama-1       |             {%- if '</think>' in content %}
llama-1       |                 {%- set reasoning_content = ((content.split('</think>')|first).rstrip('\n').split('<think>')|last).lstrip('\n') %}
llama-1       |                 {%- set content = (content.split('</think>')|last).lstrip('\n') %}
llama-1       |             {%- endif %}
llama-1       |         {%- endif %}
llama-1       |         {%- if loop.index0 > ns.last_query_index %}
llama-1       |             {%- if loop.last or (not loop.last and reasoning_content) %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
llama-1       |             {%- else %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |             {%- endif %}
llama-1       |         {%- else %}
llama-1       |             {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |         {%- endif %}
llama-1       |         {%- if message.tool_calls %}
llama-1       |             {%- for tool_call in message.tool_calls %}
llama-1       |                 {%- if (loop.first and content) or (not loop.first) %}
llama-1       |                     {{- '\n' }}
llama-1       |                 {%- endif %}
llama-1       |                 {%- if tool_call.function %}
llama-1       |                     {%- set tool_call = tool_call.function %}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '<tool_call>\n{"name": "' }}
llama-1       |                 {{- tool_call.name }}
llama-1       |                 {{- '", "arguments": ' }}
llama-1       |                 {%- if tool_call.arguments is string %}
llama-1       |                     {{- tool_call.arguments }}
llama-1       |                 {%- else %}
llama-1       |                     {{- tool_call.arguments | tojson }}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '}\n</tool_call>' }}
llama-1       |             {%- endfor %}
llama-1       |         {%- endif %}
llama-1       |         {{- '<|im_end|>\n' }}
llama-1       |     {%- elif message.role == "tool" %}
llama-1       |         {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
llama-1       |             {{- '<|im_start|>user' }}
llama-1       |         {%- endif %}
llama-1       |         {{- '\n<tool_response>\n' }}
llama-1       |         {{- content }}
llama-1       |         {{- '\n</tool_response>' }}
llama-1       |         {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
llama-1       |             {{- '<|im_end|>\n' }}
llama-1       |         {%- endif %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- if add_generation_prompt %}
llama-1       |     {{- '<|im_start|>assistant\n' }}
llama-1       | {%- endif %}, example_format: '<|im_start|>system
llama-1       | You are a helpful assistant<|im_end|>
llama-1       | <|im_start|>user
llama-1       | Hello<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | Hi there<|im_end|>
llama-1       | <|im_start|>user
llama-1       | How are you?<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | '
llama-1       | main: server is listening on http://0.0.0.0:8080 - starting the main loop
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 0 | processing task
llama-1       | slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 526
llama-1       | slot update_slots: id  0 | task 0 | kv cache rm [0, end)
llama-1       | slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 526, n_tokens = 526, progress = 1.000000
llama-1       | slot update_slots: id  0 | task 0 | prompt done, n_past = 526, n_tokens = 526
llama-1       | srv    operator(): operator(): cleaning up before exit...
llama-1       | libggml-base.so(+0x16a5b)[0x7fe12c7d9a5b]
llama-1       | libggml-base.so(ggml_print_backtrace+0x21c)[0x7fe12c7d9ebc]
llama-1       | libggml-base.so(+0x286bf)[0x7fe12c7eb6bf]
llama-1       | /lib/x86_64-linux-gnu/libstdc++.so.6(+0xbb0da)[0x7fe12c5fe0da]
llama-1       | /lib/x86_64-linux-gnu/libstdc++.so.6(_ZSt10unexpectedv+0x0)[0x7fe12c5e8a55]
llama-1       | /app/llama-server(+0xc8bda)[0x55b469877bda]
llama-1       | /app/llama-server(+0x60e87)[0x55b46980fe87]
llama-1       | /app/llama-server(+0x79de3)[0x55b469828de3]
llama-1       | /app/llama-server(+0xb7211)[0x55b469866211]
llama-1       | /app/llama-server(+0x1090b1)[0x55b4698b80b1]
llama-1       | /app/llama-server(+0x10b076)[0x55b4698ba076]
llama-1       | /app/llama-server(+0x89da2)[0x55b469838da2]
llama-1       | /lib/x86_64-linux-gnu/libstdc++.so.6(+0xecdb4)[0x7fe12c62fdb4]
llama-1       | /lib/x86_64-linux-gnu/libc.so.6(+0x9caa4)[0x7fe12c2b6aa4]
llama-1       | /lib/x86_64-linux-gnu/libc.so.6(__clone+0x44)[0x7fe12c343a34]
llama-1       | terminate called without an active exception
llama-1       | ggml_vulkan: No devices found.
llama-1       | load_backend: loaded Vulkan backend from /app/libggml-vulkan.so
llama-1       | load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
llama-1       | build: 6265 (c247d06f) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu
llama-1       | system info: n_threads = 6, n_threads_batch = 6, total_threads = 12
llama-1       | 
llama-1       | system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
llama-1       | 
llama-1       | main: binding port with default address family
llama-1       | main: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 11
llama-1       | main: loading model
llama-1       | srv    load_model: loading model '/models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf'
llama-1       | llama_model_loader: loaded meta data with 42 key-value pairs and 398 tensors from /models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf (version GGUF V3 (latest))
llama-1       | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama-1       | llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama-1       | llama_model_loader: - kv   1:                               general.type str              = model
llama-1       | llama_model_loader: - kv   2:                               general.name str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   3:                            general.version str              = 2507
llama-1       | llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama-1       | llama_model_loader: - kv   5:                           general.basename str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth
llama-1       | llama_model_loader: - kv   7:                         general.size_label str              = 4B
llama-1       | llama_model_loader: - kv   8:                            general.license str              = apache-2.0
llama-1       | llama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  10:                           general.repo_url str              = https://huggingface.co/unsloth
llama-1       | llama_model_loader: - kv  11:                   general.base_model.count u32              = 1
llama-1       | llama_model_loader: - kv  12:                  general.base_model.0.name str              = Qwen3 4B Instruct 2507
llama-1       | llama_model_loader: - kv  13:               general.base_model.0.version str              = 2507
llama-1       | llama_model_loader: - kv  14:          general.base_model.0.organization str              = Qwen
llama-1       | llama_model_loader: - kv  15:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  16:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
llama-1       | llama_model_loader: - kv  17:                          qwen3.block_count u32              = 36
llama-1       | llama_model_loader: - kv  18:                       qwen3.context_length u32              = 262144
llama-1       | llama_model_loader: - kv  19:                     qwen3.embedding_length u32              = 2560
llama-1       | llama_model_loader: - kv  20:                  qwen3.feed_forward_length u32              = 9728
llama-1       | llama_model_loader: - kv  21:                 qwen3.attention.head_count u32              = 32
llama-1       | llama_model_loader: - kv  22:              qwen3.attention.head_count_kv u32              = 8
llama-1       | llama_model_loader: - kv  23:                       qwen3.rope.freq_base f32              = 5000000.000000
llama-1       | llama_model_loader: - kv  24:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama-1       | llama_model_loader: - kv  25:                 qwen3.attention.key_length u32              = 128
llama-1       | llama_model_loader: - kv  26:               qwen3.attention.value_length u32              = 128
llama-1       | llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama-1       | llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama-1       | llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama-1       | llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama-1       | llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama-1       | llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645
llama-1       | llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151654
llama-1       | llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false
llama-1       | llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama-1       | llama_model_loader: - kv  36:               general.quantization_version u32              = 2
llama-1       | llama_model_loader: - kv  37:                          general.file_type u32              = 14
llama-1       | llama_model_loader: - kv  38:                      quantize.imatrix.file str              = Qwen3-4B-Instruct-2507-GGUF/imatrix_u...
llama-1       | llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-4B-Instruct...
llama-1       | llama_model_loader: - kv  40:             quantize.imatrix.entries_count u32              = 252
llama-1       | llama_model_loader: - kv  41:              quantize.imatrix.chunks_count u32              = 79
llama-1       | llama_model_loader: - type  f32:  145 tensors
llama-1       | llama_model_loader: - type q4_K:  244 tensors
llama-1       | llama_model_loader: - type q5_K:    8 tensors
llama-1       | llama_model_loader: - type q6_K:    1 tensors
llama-1       | print_info: file format = GGUF V3 (latest)
llama-1       | print_info: file type   = Q4_K - Small
llama-1       | print_info: file size   = 2.21 GiB (4.73 BPW) 
llama-1       | load: printing all EOG tokens:
llama-1       | load:   - 151643 ('<|endoftext|>')
llama-1       | load:   - 151645 ('<|im_end|>')
llama-1       | load:   - 151662 ('<|fim_pad|>')
llama-1       | load:   - 151663 ('<|repo_name|>')
llama-1       | load:   - 151664 ('<|file_sep|>')
llama-1       | load: special tokens cache size = 26
llama-1       | load: token to piece cache size = 0.9311 MB
llama-1       | print_info: arch             = qwen3
llama-1       | print_info: vocab_only       = 0
llama-1       | print_info: n_ctx_train      = 262144
llama-1       | print_info: n_embd           = 2560
llama-1       | print_info: n_layer          = 36
llama-1       | print_info: n_head           = 32
llama-1       | print_info: n_head_kv        = 8
llama-1       | print_info: n_rot            = 128
llama-1       | print_info: n_swa            = 0
llama-1       | print_info: is_swa_any       = 0
llama-1       | print_info: n_embd_head_k    = 128
llama-1       | print_info: n_embd_head_v    = 128
llama-1       | print_info: n_gqa            = 4
llama-1       | print_info: n_embd_k_gqa     = 1024
llama-1       | print_info: n_embd_v_gqa     = 1024
llama-1       | print_info: f_norm_eps       = 0.0e+00
llama-1       | print_info: f_norm_rms_eps   = 1.0e-06
llama-1       | print_info: f_clamp_kqv      = 0.0e+00
llama-1       | print_info: f_max_alibi_bias = 0.0e+00
llama-1       | print_info: f_logit_scale    = 0.0e+00
llama-1       | print_info: f_attn_scale     = 0.0e+00
llama-1       | print_info: n_ff             = 9728
llama-1       | print_info: n_expert         = 0
llama-1       | print_info: n_expert_used    = 0
llama-1       | print_info: causal attn      = 1
llama-1       | print_info: pooling type     = -1
llama-1       | print_info: rope type        = 2
llama-1       | print_info: rope scaling     = linear
llama-1       | print_info: freq_base_train  = 5000000.0
llama-1       | print_info: freq_scale_train = 1
llama-1       | print_info: n_ctx_orig_yarn  = 262144
llama-1       | print_info: rope_finetuned   = unknown
llama-1       | print_info: model type       = 4B
llama-1       | print_info: model params     = 4.02 B
llama-1       | print_info: general.name     = Qwen3-4B-Instruct-2507
llama-1       | print_info: vocab type       = BPE
llama-1       | print_info: n_vocab          = 151936
llama-1       | print_info: n_merges         = 151387
llama-1       | print_info: BOS token        = 11 ','
llama-1       | print_info: EOS token        = 151645 '<|im_end|>'
llama-1       | print_info: EOT token        = 151645 '<|im_end|>'
llama-1       | print_info: PAD token        = 151654 '<|vision_pad|>'
llama-1       | print_info: LF token         = 198 'Ċ'
llama-1       | print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
llama-1       | print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
llama-1       | print_info: FIM MID token    = 151660 '<|fim_middle|>'
llama-1       | print_info: FIM PAD token    = 151662 '<|fim_pad|>'
llama-1       | print_info: FIM REP token    = 151663 '<|repo_name|>'
llama-1       | print_info: FIM SEP token    = 151664 '<|file_sep|>'
llama-1       | print_info: EOG token        = 151643 '<|endoftext|>'
llama-1       | print_info: EOG token        = 151645 '<|im_end|>'
llama-1       | print_info: EOG token        = 151662 '<|fim_pad|>'
llama-1       | print_info: EOG token        = 151663 '<|repo_name|>'
llama-1       | print_info: EOG token        = 151664 '<|file_sep|>'
llama-1       | print_info: max token length = 256
llama-1       | load_tensors: loading model tensors, this can take a while... (mmap = true)
llama-1       | load_tensors:   CPU_REPACK model buffer size =  1890.00 MiB
llama-1       | load_tensors:   CPU_Mapped model buffer size =  2253.86 MiB
llama-1       | ........................................................................................
llama-1       | llama_context: constructing llama_context
llama-1       | llama_context: n_seq_max     = 1
llama-1       | llama_context: n_ctx         = 4096
llama-1       | llama_context: n_ctx_per_seq = 4096
llama-1       | llama_context: n_batch       = 2048
llama-1       | llama_context: n_ubatch      = 512
llama-1       | llama_context: causal_attn   = 1
llama-1       | llama_context: flash_attn    = 0
llama-1       | llama_context: kv_unified    = false
llama-1       | llama_context: freq_base     = 5000000.0
llama-1       | llama_context: freq_scale    = 1
llama-1       | llama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
llama-1       | llama_context:        CPU  output buffer size =     0.58 MiB
llama-1       | llama_kv_cache:        CPU KV buffer size =   576.00 MiB
llama-1       | llama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama-1       | llama_context:        CPU compute buffer size =   301.75 MiB
llama-1       | llama_context: graph nodes  = 1410
llama-1       | llama_context: graph splits = 1
llama-1       | common_init_from_params: added <|endoftext|> logit bias = -inf
llama-1       | common_init_from_params: added <|im_end|> logit bias = -inf
llama-1       | common_init_from_params: added <|fim_pad|> logit bias = -inf
llama-1       | common_init_from_params: added <|repo_name|> logit bias = -inf
llama-1       | common_init_from_params: added <|file_sep|> logit bias = -inf
llama-1       | common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
llama-1       | common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
llama-1       | srv          init: initializing slots, n_slots = 1
llama-1       | slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
llama-1       | main: model loaded
llama-1       | main: chat template, chat_template: {%- if tools %}
llama-1       |     {{- '<|im_start|>system\n' }}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- messages[0].content + '\n\n' }}
llama-1       |     {%- endif %}
llama-1       |     {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
llama-1       |     {%- for tool in tools %}
llama-1       |         {{- "\n" }}
llama-1       |         {{- tool | tojson }}
llama-1       |     {%- endfor %}
llama-1       |     {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
llama-1       | {%- else %}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
llama-1       |     {%- endif %}
llama-1       | {%- endif %}
llama-1       | {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
llama-1       | {%- for message in messages[::-1] %}
llama-1       |     {%- set index = (messages|length - 1) - loop.index0 %}
llama-1       |     {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
llama-1       |         {%- set ns.multi_step_tool = false %}
llama-1       |         {%- set ns.last_query_index = index %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- for message in messages %}
llama-1       |     {%- if message.content is string %}
llama-1       |         {%- set content = message.content %}
llama-1       |     {%- else %}
llama-1       |         {%- set content = '' %}
llama-1       |     {%- endif %}
llama-1       |     {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
llama-1       |         {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
llama-1       |     {%- elif message.role == "assistant" %}
llama-1       |         {%- set reasoning_content = '' %}
llama-1       |         {%- if message.reasoning_content is string %}
llama-1       |             {%- set reasoning_content = message.reasoning_content %}
llama-1       |         {%- else %}
llama-1       |             {%- if '</think>' in content %}
llama-1       |                 {%- set reasoning_content = ((content.split('</think>')|first).rstrip('\n').split('<think>')|last).lstrip('\n') %}
llama-1       |                 {%- set content = (content.split('</think>')|last).lstrip('\n') %}
llama-1       |             {%- endif %}
llama-1       |         {%- endif %}
llama-1       |         {%- if loop.index0 > ns.last_query_index %}
llama-1       |             {%- if loop.last or (not loop.last and reasoning_content) %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
llama-1       |             {%- else %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |             {%- endif %}
llama-1       |         {%- else %}
llama-1       |             {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |         {%- endif %}
llama-1       |         {%- if message.tool_calls %}
llama-1       |             {%- for tool_call in message.tool_calls %}
llama-1       |                 {%- if (loop.first and content) or (not loop.first) %}
llama-1       |                     {{- '\n' }}
llama-1       |                 {%- endif %}
llama-1       |                 {%- if tool_call.function %}
llama-1       |                     {%- set tool_call = tool_call.function %}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '<tool_call>\n{"name": "' }}
llama-1       |                 {{- tool_call.name }}
llama-1       |                 {{- '", "arguments": ' }}
llama-1       |                 {%- if tool_call.arguments is string %}
llama-1       |                     {{- tool_call.arguments }}
llama-1       |                 {%- else %}
llama-1       |                     {{- tool_call.arguments | tojson }}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '}\n</tool_call>' }}
llama-1       |             {%- endfor %}
llama-1       |         {%- endif %}
llama-1       |         {{- '<|im_end|>\n' }}
llama-1       |     {%- elif message.role == "tool" %}
llama-1       |         {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
llama-1       |             {{- '<|im_start|>user' }}
llama-1       |         {%- endif %}
llama-1       |         {{- '\n<tool_response>\n' }}
llama-1       |         {{- content }}
llama-1       |         {{- '\n</tool_response>' }}
llama-1       |         {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
llama-1       |             {{- '<|im_end|>\n' }}
llama-1       |         {%- endif %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- if add_generation_prompt %}
llama-1       |     {{- '<|im_start|>assistant\n' }}
llama-1       | {%- endif %}, example_format: '<|im_start|>system
llama-1       | You are a helpful assistant<|im_end|>
llama-1       | <|im_start|>user
llama-1       | Hello<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | Hi there<|im_end|>
llama-1       | <|im_start|>user
llama-1       | How are you?<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | '
llama-1       | main: server is listening on http://0.0.0.0:8080 - starting the main loop
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 0 | processing task
llama-1       | slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 526
llama-1       | slot update_slots: id  0 | task 0 | kv cache rm [0, end)
llama-1       | slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 526, n_tokens = 526, progress = 1.000000
llama-1       | slot update_slots: id  0 | task 0 | prompt done, n_past = 526, n_tokens = 526
llama-1       | slot      release: id  0 | task 0 | stop processing: n_past = 537, truncated = 0
llama-1       | slot print_timing: id  0 | task 0 | 
llama-1       | prompt eval time =    8735.20 ms /   526 tokens (   16.61 ms per token,    60.22 tokens per second)
llama-1       |        eval time =    1065.22 ms /    12 tokens (   88.77 ms per token,    11.27 tokens per second)
llama-1       |       total time =    9800.42 ms /   538 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.2 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 13 | processing task
llama-1       | slot update_slots: id  0 | task 13 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 528
llama-1       | slot update_slots: id  0 | task 13 | kv cache rm [520, end)
llama-1       | slot update_slots: id  0 | task 13 | prompt processing progress, n_past = 528, n_tokens = 8, progress = 0.015152
llama-1       | slot update_slots: id  0 | task 13 | prompt done, n_past = 528, n_tokens = 8
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | slot      release: id  0 | task 13 | stop processing: n_past = 548, truncated = 0
llama-1       | slot print_timing: id  0 | task 13 | 
llama-1       | prompt eval time =     204.81 ms /     8 tokens (   25.60 ms per token,    39.06 tokens per second)
llama-1       |        eval time =    1878.30 ms /    21 tokens (   89.44 ms per token,    11.18 tokens per second)
llama-1       |       total time =    2083.10 ms /    29 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.2 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 35 | processing task
llama-1       | slot update_slots: id  0 | task 35 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 561
llama-1       | slot update_slots: id  0 | task 35 | kv cache rm [548, end)
llama-1       | slot update_slots: id  0 | task 35 | prompt processing progress, n_past = 561, n_tokens = 13, progress = 0.023173
llama-1       | slot update_slots: id  0 | task 35 | prompt done, n_past = 561, n_tokens = 13
llama-1       | slot      release: id  0 | task 35 | stop processing: n_past = 592, truncated = 0
llama-1       | slot print_timing: id  0 | task 35 | 
llama-1       | prompt eval time =     330.40 ms /    13 tokens (   25.42 ms per token,    39.35 tokens per second)
llama-1       |        eval time =    3005.83 ms /    32 tokens (   93.93 ms per token,    10.65 tokens per second)
llama-1       |       total time =    3336.23 ms /    45 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.2 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv    operator(): operator(): cleaning up before exit...
llama-1       | ggml_vulkan: No devices found.
llama-1       | load_backend: loaded Vulkan backend from /app/libggml-vulkan.so
llama-1       | load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
llama-1       | build: 6265 (c247d06f) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu
llama-1       | system info: n_threads = 6, n_threads_batch = 6, total_threads = 12
llama-1       | 
llama-1       | system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
llama-1       | 
llama-1       | main: binding port with default address family
llama-1       | main: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 11
llama-1       | main: loading model
llama-1       | srv    load_model: loading model '/models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf'
llama-1       | llama_model_loader: loaded meta data with 42 key-value pairs and 398 tensors from /models/Qwen3-4B-Instruct-2507-Q4_K_S.gguf (version GGUF V3 (latest))
llama-1       | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama-1       | llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama-1       | llama_model_loader: - kv   1:                               general.type str              = model
llama-1       | llama_model_loader: - kv   2:                               general.name str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   3:                            general.version str              = 2507
llama-1       | llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama-1       | llama_model_loader: - kv   5:                           general.basename str              = Qwen3-4B-Instruct-2507
llama-1       | llama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth
llama-1       | llama_model_loader: - kv   7:                         general.size_label str              = 4B
llama-1       | llama_model_loader: - kv   8:                            general.license str              = apache-2.0
llama-1       | llama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  10:                           general.repo_url str              = https://huggingface.co/unsloth
llama-1       | llama_model_loader: - kv  11:                   general.base_model.count u32              = 1
llama-1       | llama_model_loader: - kv  12:                  general.base_model.0.name str              = Qwen3 4B Instruct 2507
llama-1       | llama_model_loader: - kv  13:               general.base_model.0.version str              = 2507
llama-1       | llama_model_loader: - kv  14:          general.base_model.0.organization str              = Qwen
llama-1       | llama_model_loader: - kv  15:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-4B-...
llama-1       | llama_model_loader: - kv  16:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
llama-1       | llama_model_loader: - kv  17:                          qwen3.block_count u32              = 36
llama-1       | llama_model_loader: - kv  18:                       qwen3.context_length u32              = 262144
llama-1       | llama_model_loader: - kv  19:                     qwen3.embedding_length u32              = 2560
llama-1       | llama_model_loader: - kv  20:                  qwen3.feed_forward_length u32              = 9728
llama-1       | llama_model_loader: - kv  21:                 qwen3.attention.head_count u32              = 32
llama-1       | llama_model_loader: - kv  22:              qwen3.attention.head_count_kv u32              = 8
llama-1       | llama_model_loader: - kv  23:                       qwen3.rope.freq_base f32              = 5000000.000000
llama-1       | llama_model_loader: - kv  24:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama-1       | llama_model_loader: - kv  25:                 qwen3.attention.key_length u32              = 128
llama-1       | llama_model_loader: - kv  26:               qwen3.attention.value_length u32              = 128
llama-1       | llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama-1       | llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama-1       | llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama-1       | llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama-1       | llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama-1       | llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645
llama-1       | llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151654
llama-1       | llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false
llama-1       | llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama-1       | llama_model_loader: - kv  36:               general.quantization_version u32              = 2
llama-1       | llama_model_loader: - kv  37:                          general.file_type u32              = 14
llama-1       | llama_model_loader: - kv  38:                      quantize.imatrix.file str              = Qwen3-4B-Instruct-2507-GGUF/imatrix_u...
llama-1       | llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-4B-Instruct...
llama-1       | llama_model_loader: - kv  40:             quantize.imatrix.entries_count u32              = 252
llama-1       | llama_model_loader: - kv  41:              quantize.imatrix.chunks_count u32              = 79
llama-1       | llama_model_loader: - type  f32:  145 tensors
llama-1       | llama_model_loader: - type q4_K:  244 tensors
llama-1       | llama_model_loader: - type q5_K:    8 tensors
llama-1       | llama_model_loader: - type q6_K:    1 tensors
llama-1       | print_info: file format = GGUF V3 (latest)
llama-1       | print_info: file type   = Q4_K - Small
llama-1       | print_info: file size   = 2.21 GiB (4.73 BPW) 
llama-1       | load: printing all EOG tokens:
llama-1       | load:   - 151643 ('<|endoftext|>')
llama-1       | load:   - 151645 ('<|im_end|>')
llama-1       | load:   - 151662 ('<|fim_pad|>')
llama-1       | load:   - 151663 ('<|repo_name|>')
llama-1       | load:   - 151664 ('<|file_sep|>')
llama-1       | load: special tokens cache size = 26
llama-1       | load: token to piece cache size = 0.9311 MB
llama-1       | print_info: arch             = qwen3
llama-1       | print_info: vocab_only       = 0
llama-1       | print_info: n_ctx_train      = 262144
llama-1       | print_info: n_embd           = 2560
llama-1       | print_info: n_layer          = 36
llama-1       | print_info: n_head           = 32
llama-1       | print_info: n_head_kv        = 8
llama-1       | print_info: n_rot            = 128
llama-1       | print_info: n_swa            = 0
llama-1       | print_info: is_swa_any       = 0
llama-1       | print_info: n_embd_head_k    = 128
llama-1       | print_info: n_embd_head_v    = 128
llama-1       | print_info: n_gqa            = 4
llama-1       | print_info: n_embd_k_gqa     = 1024
llama-1       | print_info: n_embd_v_gqa     = 1024
llama-1       | print_info: f_norm_eps       = 0.0e+00
llama-1       | print_info: f_norm_rms_eps   = 1.0e-06
llama-1       | print_info: f_clamp_kqv      = 0.0e+00
llama-1       | print_info: f_max_alibi_bias = 0.0e+00
llama-1       | print_info: f_logit_scale    = 0.0e+00
llama-1       | print_info: f_attn_scale     = 0.0e+00
llama-1       | print_info: n_ff             = 9728
llama-1       | print_info: n_expert         = 0
llama-1       | print_info: n_expert_used    = 0
llama-1       | print_info: causal attn      = 1
llama-1       | print_info: pooling type     = -1
llama-1       | print_info: rope type        = 2
llama-1       | print_info: rope scaling     = linear
llama-1       | print_info: freq_base_train  = 5000000.0
llama-1       | print_info: freq_scale_train = 1
llama-1       | print_info: n_ctx_orig_yarn  = 262144
llama-1       | print_info: rope_finetuned   = unknown
llama-1       | print_info: model type       = 4B
llama-1       | print_info: model params     = 4.02 B
llama-1       | print_info: general.name     = Qwen3-4B-Instruct-2507
llama-1       | print_info: vocab type       = BPE
llama-1       | print_info: n_vocab          = 151936
llama-1       | print_info: n_merges         = 151387
llama-1       | print_info: BOS token        = 11 ','
llama-1       | print_info: EOS token        = 151645 '<|im_end|>'
llama-1       | print_info: EOT token        = 151645 '<|im_end|>'
llama-1       | print_info: PAD token        = 151654 '<|vision_pad|>'
llama-1       | print_info: LF token         = 198 'Ċ'
llama-1       | print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
llama-1       | print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
llama-1       | print_info: FIM MID token    = 151660 '<|fim_middle|>'
llama-1       | print_info: FIM PAD token    = 151662 '<|fim_pad|>'
llama-1       | print_info: FIM REP token    = 151663 '<|repo_name|>'
llama-1       | print_info: FIM SEP token    = 151664 '<|file_sep|>'
llama-1       | print_info: EOG token        = 151643 '<|endoftext|>'
llama-1       | print_info: EOG token        = 151645 '<|im_end|>'
llama-1       | print_info: EOG token        = 151662 '<|fim_pad|>'
llama-1       | print_info: EOG token        = 151663 '<|repo_name|>'
llama-1       | print_info: EOG token        = 151664 '<|file_sep|>'
llama-1       | print_info: max token length = 256
llama-1       | load_tensors: loading model tensors, this can take a while... (mmap = true)
llama-1       | load_tensors:   CPU_REPACK model buffer size =  1890.00 MiB
llama-1       | load_tensors:   CPU_Mapped model buffer size =  2253.86 MiB
llama-1       | ........................................................................................
llama-1       | llama_context: constructing llama_context
llama-1       | llama_context: n_seq_max     = 1
llama-1       | llama_context: n_ctx         = 4096
llama-1       | llama_context: n_ctx_per_seq = 4096
llama-1       | llama_context: n_batch       = 2048
llama-1       | llama_context: n_ubatch      = 512
llama-1       | llama_context: causal_attn   = 1
llama-1       | llama_context: flash_attn    = 0
llama-1       | llama_context: kv_unified    = false
llama-1       | llama_context: freq_base     = 5000000.0
llama-1       | llama_context: freq_scale    = 1
llama-1       | llama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
llama-1       | llama_context:        CPU  output buffer size =     0.58 MiB
llama-1       | llama_kv_cache:        CPU KV buffer size =   576.00 MiB
llama-1       | llama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama-1       | llama_context:        CPU compute buffer size =   301.75 MiB
llama-1       | llama_context: graph nodes  = 1410
llama-1       | llama_context: graph splits = 1
llama-1       | common_init_from_params: added <|endoftext|> logit bias = -inf
llama-1       | common_init_from_params: added <|im_end|> logit bias = -inf
llama-1       | common_init_from_params: added <|fim_pad|> logit bias = -inf
llama-1       | common_init_from_params: added <|repo_name|> logit bias = -inf
llama-1       | common_init_from_params: added <|file_sep|> logit bias = -inf
llama-1       | common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
llama-1       | common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
llama-1       | srv          init: initializing slots, n_slots = 1
llama-1       | slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
llama-1       | main: model loaded
llama-1       | main: chat template, chat_template: {%- if tools %}
llama-1       |     {{- '<|im_start|>system\n' }}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- messages[0].content + '\n\n' }}
llama-1       |     {%- endif %}
llama-1       |     {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
llama-1       |     {%- for tool in tools %}
llama-1       |         {{- "\n" }}
llama-1       |         {{- tool | tojson }}
llama-1       |     {%- endfor %}
llama-1       |     {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
llama-1       | {%- else %}
llama-1       |     {%- if messages[0].role == 'system' %}
llama-1       |         {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
llama-1       |     {%- endif %}
llama-1       | {%- endif %}
llama-1       | {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
llama-1       | {%- for message in messages[::-1] %}
llama-1       |     {%- set index = (messages|length - 1) - loop.index0 %}
llama-1       |     {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
llama-1       |         {%- set ns.multi_step_tool = false %}
llama-1       |         {%- set ns.last_query_index = index %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- for message in messages %}
llama-1       |     {%- if message.content is string %}
llama-1       |         {%- set content = message.content %}
llama-1       |     {%- else %}
llama-1       |         {%- set content = '' %}
llama-1       |     {%- endif %}
llama-1       |     {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
llama-1       |         {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
llama-1       |     {%- elif message.role == "assistant" %}
llama-1       |         {%- set reasoning_content = '' %}
llama-1       |         {%- if message.reasoning_content is string %}
llama-1       |             {%- set reasoning_content = message.reasoning_content %}
llama-1       |         {%- else %}
llama-1       |             {%- if '</think>' in content %}
llama-1       |                 {%- set reasoning_content = ((content.split('</think>')|first).rstrip('\n').split('<think>')|last).lstrip('\n') %}
llama-1       |                 {%- set content = (content.split('</think>')|last).lstrip('\n') %}
llama-1       |             {%- endif %}
llama-1       |         {%- endif %}
llama-1       |         {%- if loop.index0 > ns.last_query_index %}
llama-1       |             {%- if loop.last or (not loop.last and reasoning_content) %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
llama-1       |             {%- else %}
llama-1       |                 {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |             {%- endif %}
llama-1       |         {%- else %}
llama-1       |             {{- '<|im_start|>' + message.role + '\n' + content }}
llama-1       |         {%- endif %}
llama-1       |         {%- if message.tool_calls %}
llama-1       |             {%- for tool_call in message.tool_calls %}
llama-1       |                 {%- if (loop.first and content) or (not loop.first) %}
llama-1       |                     {{- '\n' }}
llama-1       |                 {%- endif %}
llama-1       |                 {%- if tool_call.function %}
llama-1       |                     {%- set tool_call = tool_call.function %}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '<tool_call>\n{"name": "' }}
llama-1       |                 {{- tool_call.name }}
llama-1       |                 {{- '", "arguments": ' }}
llama-1       |                 {%- if tool_call.arguments is string %}
llama-1       |                     {{- tool_call.arguments }}
llama-1       |                 {%- else %}
llama-1       |                     {{- tool_call.arguments | tojson }}
llama-1       |                 {%- endif %}
llama-1       |                 {{- '}\n</tool_call>' }}
llama-1       |             {%- endfor %}
llama-1       |         {%- endif %}
llama-1       |         {{- '<|im_end|>\n' }}
llama-1       |     {%- elif message.role == "tool" %}
llama-1       |         {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
llama-1       |             {{- '<|im_start|>user' }}
llama-1       |         {%- endif %}
llama-1       |         {{- '\n<tool_response>\n' }}
llama-1       |         {{- content }}
llama-1       |         {{- '\n</tool_response>' }}
llama-1       |         {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
llama-1       |             {{- '<|im_end|>\n' }}
llama-1       |         {%- endif %}
llama-1       |     {%- endif %}
llama-1       | {%- endfor %}
llama-1       | {%- if add_generation_prompt %}
llama-1       |     {{- '<|im_start|>assistant\n' }}
llama-1       | {%- endif %}, example_format: '<|im_start|>system
llama-1       | You are a helpful assistant<|im_end|>
llama-1       | <|im_start|>user
llama-1       | Hello<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | Hi there<|im_end|>
llama-1       | <|im_start|>user
llama-1       | How are you?<|im_end|>
llama-1       | <|im_start|>assistant
llama-1       | '
llama-1       | main: server is listening on http://0.0.0.0:8080 - starting the main loop
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 0 | processing task
llama-1       | slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 526
llama-1       | slot update_slots: id  0 | task 0 | kv cache rm [0, end)
llama-1       | slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 526, n_tokens = 526, progress = 1.000000
llama-1       | slot update_slots: id  0 | task 0 | prompt done, n_past = 526, n_tokens = 526
llama-1       | slot      release: id  0 | task 0 | stop processing: n_past = 537, truncated = 0
llama-1       | slot print_timing: id  0 | task 0 | 
llama-1       | prompt eval time =    9239.17 ms /   526 tokens (   17.56 ms per token,    56.93 tokens per second)
llama-1       |        eval time =    1080.09 ms /    12 tokens (   90.01 ms per token,    11.11 tokens per second)
llama-1       |       total time =   10319.26 ms /   538 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.2 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 13 | processing task
llama-1       | slot update_slots: id  0 | task 13 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 528
llama-1       | slot update_slots: id  0 | task 13 | kv cache rm [520, end)
llama-1       | slot update_slots: id  0 | task 13 | prompt processing progress, n_past = 528, n_tokens = 8, progress = 0.015152
llama-1       | slot update_slots: id  0 | task 13 | prompt done, n_past = 528, n_tokens = 8
llama-1       | slot      release: id  0 | task 13 | stop processing: n_past = 548, truncated = 0
llama-1       | slot print_timing: id  0 | task 13 | 
llama-1       | prompt eval time =     206.25 ms /     8 tokens (   25.78 ms per token,    38.79 tokens per second)
llama-1       |        eval time =    1860.62 ms /    21 tokens (   88.60 ms per token,    11.29 tokens per second)
llama-1       |       total time =    2066.88 ms /    29 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.2 200
llama-1       | srv  params_from_: Chat format: Hermes 2 Pro
llama-1       | slot launch_slot_: id  0 | task 35 | processing task
llama-1       | slot update_slots: id  0 | task 35 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 561
llama-1       | slot update_slots: id  0 | task 35 | kv cache rm [548, end)
llama-1       | slot update_slots: id  0 | task 35 | prompt processing progress, n_past = 561, n_tokens = 13, progress = 0.023173
llama-1       | slot update_slots: id  0 | task 35 | prompt done, n_past = 561, n_tokens = 13
llama-1       | slot      release: id  0 | task 35 | stop processing: n_past = 592, truncated = 0
llama-1       | slot print_timing: id  0 | task 35 | 
llama-1       | prompt eval time =     303.15 ms /    13 tokens (   23.32 ms per token,    42.88 tokens per second)
llama-1       |        eval time =    2956.09 ms /    32 tokens (   92.38 ms per token,    10.83 tokens per second)
llama-1       |       total time =    3259.24 ms /    45 tokens
llama-1       | srv  update_slots: all slots are idle
llama-1       | srv  log_server_r: request: POST /v1/chat/completions 172.20.0.2 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv  log_server_r: request: GET /health 127.0.0.1 200
llama-1       | srv    operator(): operator(): cleaning up before exit...
